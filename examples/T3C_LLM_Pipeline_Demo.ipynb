{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPH8itGmd9uXeLSi1pOcceX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/staceysv/tttc-light-js/blob/main/examples/T3C_LLM_Pipeline_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talk to the City (T3C) Workflow\n",
        "\n",
        "Talk to the City (T3C) summarizes and organizes diverse human perspectives for easier analysis and decision-making.\n",
        "This notebook documents and visualizes the LLM inteface, factoring out the prompts & processing and logging everything, including intermediate stages and costs, to W&B.\n",
        "\n",
        "## LLM Prompting Pipeline\n",
        "\n",
        "1. Given all comments, create a taxonomy tree of concise topics + subtopics.\n",
        "2. For each comment, extract claims and assign them to a subtopic in the taxonomy tree.\n",
        "3. (no LLM calls) Order the subtopics by the most claims.\n",
        "4. Deduplicate claims in each subtopic.\n",
        "\n",
        "### More context\n",
        "* [T3C Github Repo](https://github.com/AIObjectives/tttc-light-js)\n",
        "* [T3C Product Overview](https://ai.objectives.institute/talk-to-the-city)\n",
        "\n",
        "### Sample CSV data\n",
        "\n",
        "* Tweets on AI safety: 100, 500, 1000, 2893\n",
        "* Reddit climate change: posts titles 100, 250, 500\n",
        "* Goodreads poetry book reviews, 500\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YbFNiYxcDMmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 Setup & imports"
      ],
      "metadata": {
        "id": "zoyB05fkdP9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.0 Import packages, auth with W&B + OAI\n",
        "\n",
        "You'll need a [W&B API key](https://www.wandb.ai/authorize) and an OpenAI key. This colab will not log or store the keys."
      ],
      "metadata": {
        "id": "EseweiVbdfOK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbYLhN8t5GHx"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install -qqq weave\n",
        "!pip install -qqq wandb\n",
        "import wandb\n",
        "import pandas as pd\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# authenticate with OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")"
      ],
      "metadata": {
        "id": "eZSuvDXidkIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Utils\n",
        "\n",
        "Helpful functions"
      ],
      "metadata": {
        "id": "2zICHE6-sxmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "\n",
        "def time_here():\n",
        "  date_format='%m/%d/%Y %H:%M:%S'\n",
        "  date = datetime.now()\n",
        "  date = date.astimezone(timezone('US/Pacific'))\n",
        "  return date.strftime(date_format)\n",
        "\n",
        "def topic_tree(taxonomy):\n",
        "  core_tree = []\n",
        "  full_tree = taxonomy[\"taxonomy\"]\n",
        "  for main_topic in full_tree:\n",
        "    topic = main_topic[\"topicName\"]\n",
        "    desc = main_topic[\"topicShortDescription\"]\n",
        "    subtopic_list = []\n",
        "    for subtopic in main_topic[\"subtopics\"]:\n",
        "      if \"subtopicName\" in subtopic:\n",
        "        sub_topic = subtopic[\"subtopicName\"]\n",
        "      else:\n",
        "        print(\"WARNING: NO TOPIC NAME\")\n",
        "        continue\n",
        "      if \"subtopicShortDescription\" in subtopic:\n",
        "        sub_desc = subtopic[\"subtopicShortDescription\"]\n",
        "      else:\n",
        "        print(\"WARNING: NO TOPIC DESCRIPTION\")\n",
        "        sub_desc = \"N/A\"\n",
        "      subtopic_list.append({sub_topic : sub_desc})\n",
        "    core_tree.append({ topic : desc, \"subtopic\" : subtopic_list})\n",
        "  return core_tree\n",
        "\n",
        "def cute_print(json_obj):\n",
        "  \"\"\"Returns a pretty version of a dictionary as properly-indented and scaled\n",
        "  json in html for at-a-glance review in W&B\"\"\"\n",
        "  str_json = json.dumps(json_obj, indent=1)\n",
        "  cute_html = '<pre id=\"json\"><font size=2>' + str_json + \"</font></pre>\"\n",
        "  return wandb.Html(cute_html)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ug8oLqRbodif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 Input data\n",
        "\n",
        "Sample lists of comments which might be useful for testing/future adhoc exploration. Load directly from code or CSV file.\n",
        "\n",
        "Testing factors to consider:\n",
        "* duplicates: very similar/identical statements within a topic or across themes/topics\n",
        "* subject/object, main point, length and clarity\n",
        "* intensity/generality of opinions"
      ],
      "metadata": {
        "id": "5L5yZ9Q1M4OE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2.0 Load comments from CSV\n",
        "\n",
        "Send the \"comments\" column from a dataframe through the pipeline and save the source data to W&B.\n"
      ],
      "metadata": {
        "id": "rJC7H_UbNsId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_FILENAME = \"../tw_1000.csv\"\n",
        "df = pd.read_csv(open(CSV_FILENAME, 'r'))\n",
        "comments = df[\"comments\"]\n",
        "\n",
        "# optionally upload to W&B\n",
        "#wandb.init(project=WB_PROJECT_NAME, name=\"upload_csv_comments\", group=\"csv_comment_upload\")\n",
        "#wandb.log({\"tiny_dream_test\" : df})\n",
        "#wandb.run.finish()"
      ],
      "metadata": {
        "id": "Q6wUVLEy5QEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2.1 Load tiny test lists from code"
      ],
      "metadata": {
        "id": "RswB6iLQTrsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# past sample comments possibly useful for testing\n",
        "MVP_TEST = [\"I love cats\", \"I really really love dogs\", \"I'm not sure about birds\"]\n",
        "#comments = [\"none\"]\n",
        "MVP_TEST.extend([\"Cats are my favorite\", \"Dogs are the best\", \"No seriously dogs are great\", \"Birds I'm hesitant about\", \"Cats can be walked outside and they don't have to\", \"Dogs need to be walked regularly, every day\", \"Dogs can be trained to perform adorable moves on verbal command\", \"Can cats be trained?\", \"Dogs and cats are both adorable and fluffy\", \"Good pets are chill\", \"Cats are fantastic\", \"A goldfish is my top choice\"])\n",
        "SCIFI_TEST = [\"My favorite fantasy novel is Name of the Wind\", \"Terra Ignota is the best scifi series of all time\", \"Idk about Kim Stanley Robinson\"]\n",
        "SCIFI_TEST.extend([\"Name of the Wind is predictable and hard to read\", \"Some of Kim Stanley Robinson is boring\", \"Terra Ignota gets slow in the middle and hard to follow\",\n",
        "            \"Ada Palmer is spectacular\", \"Becky Chambers has fantastic aliens in her work\", \"Ministry for the Future and Years of Rice and Salt are really comprehensive and compelling stories\",\n",
        "            \"Do we still talk about Lord of the Rings or Game of Thrones or is epic fantasy over\", \"What about Ted Chiang he is so good\", \"Greg Egan is really good at characters and plot and hard science\",\n",
        "            \"I never finished Accelerando\", \"Ministry for the Future is about the climate transition\", \"The climate crisis is a major theme in Ministry for the Future\", \"Ministry for the Future is about climate\"])\n",
        "\n",
        "MVP_TEST.extend([\"Lizards are scary\", \"Kittens are my favorite when they have snake-like scales\", \"Hairless cats are unique\", \"Flying lizards are majestic\", \"Kittens are so boring\"])\n",
        "\n",
        "# *************************\n",
        "# SET OR ADD COMMENTS HERE\n",
        "# *************************\n",
        "comments = SCIFI_TEST"
      ],
      "metadata": {
        "id": "kRIHBsAAM36I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total comments: \", len(comments))"
      ],
      "metadata": {
        "id": "K2PfN92-JeIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Prompts\n",
        "\n",
        "All the prompts for the pipeline"
      ],
      "metadata": {
        "id": "TyjFGgWzLZRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\"\n",
        "You are a professional research assistant. You have helped run many public consultations,\n",
        "surveys and citizen assemblies. You have good instincts when it comes to extracting interesting insights.\n",
        "You are familiar with public consultation tools like Pol.is and you understand the benefits\n",
        "for working with very clear, concise claims that other people would be able to vote on.\n",
        "\"\"\"\n",
        "\n",
        "COMMENT_TO_TREE_PROMPT = \"\"\"\n",
        "I will give you a list of comments.\n",
        "Please propose a way to organize the information contained in these comments into topics and subtopics of interest.\n",
        "Keep the topic and subtopic names very concise and use the short description to explain what the topic is about.\n",
        "\n",
        "Return a JSON object of the form {\n",
        "  \"taxonomy\": [\n",
        "    {\n",
        "      \"topicName\": string,\n",
        "      \"topicShortDescription\": string,\n",
        "      \"subtopics\": [\n",
        "        {\n",
        "          \"subtopicName\": string,\n",
        "          \"subtopicShortDescription\": string,\n",
        "        },\n",
        "        ...\n",
        "      ]\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "Now here is the list of comments:\n",
        "\"\"\"\n",
        "\n",
        "COMMENT_TO_CLAIMS = \"\"\"\n",
        "I'm going to give you a comment made by a participant and a list of topics and subtopics which have already been extracted.\n",
        "I want you to extract a list of concise claims that the participant may support.\n",
        "We are only interested in claims that can be mapped to one of the given topic and subtopic.\n",
        "The claim must be fairly general but not a platitude.\n",
        "It must be something that other people may potentially disagree with. Each claim must also be atomic.\n",
        "For each claim, please also provide a relevant quote from the transcript.\n",
        "The quote must be as concise as possible while still supporting the argument.\n",
        "The quote doesn't need to be a logical argument.\n",
        "It could also be a personal story or anecdote illustrating why the interviewee would make this claim.\n",
        "You may use \"[...]\" in the quote to skip the less interesting bits of the quote.\n",
        "/return a JSON object of the form {\n",
        "  \"claims\": [\n",
        "    {\n",
        "      \"claim\": string, // a very concise extracted claim\n",
        "      \"quote\": string // the exact quote,\n",
        "      \"topicName\": string // from the given list of topics\n",
        "      \"subtopicName\": string // from the list of subtopics\n",
        "    },\n",
        "    // ...\n",
        "  ]\n",
        "}\n",
        "\n",
        "Now here is the list of topics/subtopics:\"\"\"\n",
        "# also include in prompt:\n",
        "# append ${taxonomy}\n",
        "# comments: And then here is the comment:\"\"\"\n",
        "\n",
        "DEDUP_PROMPT = \"\"\"\n",
        "I'm going to give you a JSON object containing a list of claims with some ids.\n",
        "I want you to remove any near-duplicate claims from the list by nesting some claims under some top-level claims.\n",
        "For example, if we have 5 claims and claim 3 and 5 are similar to claim 2, we will nest claim 3 and 5 under claim 2.\n",
        "The nesting will be represented as a JSON object where the keys are the ids of the\n",
        "top-level claims and the values are lists of ids of the nested claims.\n",
        "\n",
        "Return a JSON object of the form {\n",
        "  \"nesting\": {\n",
        "    \"claimId1\": [],\n",
        "    \"claimId2\": [\"claimId3\", \"claimId5\"],\n",
        "    \"claimId4\": []\n",
        "  }\n",
        "}\n",
        "\n",
        "And now, here are the claims:\"\"\""
      ],
      "metadata": {
        "id": "8YAouT2Jftjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: Configure pipeline run\n",
        "\n",
        "W&B variables for convenience:\n",
        "* set RUN_NAME for each new pass through the pipeline\n",
        "* optionally set EXP_GROUP for each new set of experiments (easier to toggle visibility/metrics by new logic/day/coding session/etc)\n",
        "\n"
      ],
      "metadata": {
        "id": "30QoaQXjIVXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_NAME = \"tw_424\"\n",
        "EXP_GROUP = \"uc\"\n",
        "WB_PROJECT_NAME = \"t3c_pipeline\""
      ],
      "metadata": {
        "id": "Yme7kQSuUKFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maps to gpt-4-0125-preview - token costs:\n",
        "# $10/1M in, $30/1M out = $0.1/10K in, $0.3/10K out\n",
        "MODEL = \"gpt-4-turbo-preview\"\n",
        "COST_IN_PER_10K = 0.1\n",
        "COST_OUT_PER_10K = 0.3\n",
        "\n",
        "# periodically update from W&B\n",
        "AVG_TREE_LEN_TOKS = 614\n",
        "AVG_CLAIM_TOKS_OUT = 130\n",
        "AVG_TOPIC_COUNT = 12\n",
        "AVG_DEDUP_INPUT_TOK = 12\n",
        "AVG_DEDUPED_CLAIMS_FACTOR = 0.6\n",
        "\n",
        "guess_cost = 0\n",
        "actual_cost = 0\n",
        "\n",
        "# log sheet\n",
        "#name,group,time,rows,chars,guess_cost,actual_cost,1_cost,2_cost,4_cost,num_themes,num_topics,num_claims\n",
        "#eventually we want these more..granular/averaged?"
      ],
      "metadata": {
        "id": "hrzvigtptfsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Estimate costs\n",
        "\n",
        "Estimate costs for each step and overall:\n",
        "* 1: SYS_PROMPT + COMMENTS_TO_TREE_PROMPT + len(comments)\n",
        "* 2: count(comments) * (SYS_PROMPT + COMMENT_TO_CLAIMS_PROMPT + taxonomy output of 1) + len(comments)\n",
        "* 3: (SYS_PROMPT + DEDUP_PROMPT) * len(topics) where > 1 claim\n",
        "\n",
        "1 token ~= 4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words)."
      ],
      "metadata": {
        "id": "12UZJlxObl2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimate cost before completing template\n",
        "comments_total = sum([len(c) for c in comments])\n",
        "N_sys_prompt = len(SYS_PROMPT)\n",
        "\n",
        "# set topic count to be square root of comment length?\n",
        "\n",
        "# inputs only\n",
        "step_1_cost = (N_sys_prompt + len(COMMENT_TO_TREE_PROMPT) + comments_total) / 4.0\n",
        "step_2_cost = (comments_total/4.0) + len(comments) * (((N_sys_prompt + len(COMMENT_TO_CLAIMS))/ 4.0) + AVG_TREE_LEN_TOKS)\n",
        "step_4_cost = ((N_sys_prompt + len(DEDUP_PROMPT)) /4.0) * (len(comments) ** 0.33) * AVG_DEDUP_INPUT_TOK\n",
        "total_cost = step_1_cost + step_2_cost + step_4_cost\n",
        "\n",
        "# then we divide by 10K and multiply by constants\n",
        "\n",
        "print(step_1_cost)\n",
        "print(step_2_cost)\n",
        "print(step_4_cost)\n",
        "print(total_cost)\n",
        "\n",
        "cost_in = (total_cost * COST_IN_PER_10K) / 10000.0\n",
        "\n",
        "# outputs — how do we estimate this?\n",
        "step_1_out_cost = AVG_TREE_LEN_TOKS\n",
        "step_2_out_cost = len(comments) * AVG_CLAIM_TOKS_OUT\n",
        "step_4_out_cost = len(comments) * AVG_DEDUPED_CLAIMS_FACTOR\n",
        "\n",
        "cost_out = step_1_out_cost + step_2_out_cost + step_4_out_cost\n",
        "print(step_1_out_cost)\n",
        "print(step_2_out_cost)\n",
        "print(step_4_out_cost)\n",
        "print(cost_out)\n",
        "\n",
        "total_cost_out = (cost_out * COST_OUT_PER_10K) / 10000.0\n",
        "print(\"IN: \", cost_in, \" OUT: \", total_cost_out)\n",
        "net_cost = cost_in + total_cost_out\n",
        "print(\"TOTAL GUESS: $\", net_cost)\n",
        "guess_cost = net_cost"
      ],
      "metadata": {
        "id": "uVkAp0PBeoUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Run pipeline"
      ],
      "metadata": {
        "id": "WFBakBgKe9wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Comments to tree\n",
        "\n",
        "Given the full list of comments, call LLM to create a taxonomy of main topics (themes) and subtopics with short descriptions."
      ],
      "metadata": {
        "id": "eyrJtqJyLzwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "import wandb\n",
        "import weave\n",
        "import json\n",
        "\n",
        "weave.init(WB_PROJECT_NAME)\n",
        "wandb.init(project = WB_PROJECT_NAME, name=RUN_NAME, group=EXP_GROUP,\n",
        "           # TODO: add more config here\n",
        "           config={\"model\" : MODEL,\n",
        "                   \"$_in_10K\" : COST_IN_PER_10K,\n",
        "                   \"$_out_10K\" : COST_OUT_PER_10K,\n",
        "                   \"cost_guess\" : guess_cost\n",
        "                  })\n",
        "\n",
        "# track token counts+costs for pipeline\n",
        "TK_TOT = 0\n",
        "TK_IN = 0\n",
        "TK_OUT = 0\n",
        "NUM_THEMES = 0\n",
        "\n",
        "@weave.op()\n",
        "def comments_to_tree(comments:list)-> dict:\n",
        "    client = OpenAI()\n",
        "\n",
        "    # append comments to prompt\n",
        "    full_prompt = COMMENT_TO_TREE_PROMPT\n",
        "    for comment in comments:\n",
        "      full_prompt += \"\\n\" + comment\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYS_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": full_prompt\n",
        "        }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "    tree = response.choices[0].message.content\n",
        "    return {\"tree\" : json.loads(tree), \"usage\" : response.usage}\n",
        "\n",
        "# estimate cost before completing template\n",
        "comment_lengths = [len(c) for c in comments]\n",
        "wandb.log({\"comm_N\" : len(comments), \"comm_bins\" : comment_lengths})\n",
        "\n",
        "with weave.attributes({\"model\" : MODEL, \"stage\" : \"1_comments_to_tree\"}):\n",
        "  resp = comments_to_tree(comments)\n",
        "  taxonomy = resp[\"tree\"]\n",
        "  usage = resp[\"usage\"]\n",
        "  print(taxonomy)\n",
        "  print(usage)\n",
        "  NUM_THEMES = len(taxonomy[\"taxonomy\"])\n",
        "\n",
        "# in case comments are empty / for W&B Table logging\n",
        "comment_list = \"none\"\n",
        "if len(comments) > 1:\n",
        "  comment_list = \"\\n\".join(comments)\n",
        "tl = [[comment_list, cute_print(topic_tree(taxonomy)), json.dumps(taxonomy,indent=1)]]\n",
        "\n",
        "# update token counts\n",
        "TK_TOT += usage.total_tokens\n",
        "TK_IN += usage.prompt_tokens\n",
        "TK_OUT += usage.completion_tokens\n",
        "\n",
        "# calculate and log actual cost here?\n",
        "actual_1_cost = (COST_IN_PER_10K * TK_IN + COST_OUT_PER_10K * TK_OUT) / 10000.0\n",
        "print(\"actual$:\", actual_1_cost)\n",
        "actual_cost += actual_1_cost\n",
        "\n",
        "wandb.log({\n",
        "    \"u/1/N_tok\": usage.total_tokens,\n",
        "    \"u/1/in_tok\" : usage.prompt_tokens,\n",
        "    \"u/1/out_tok\": usage.completion_tokens,\n",
        "    \"u/1/cost\" : actual_1_cost,\n",
        "    \"u/N/N_tok\" : TK_TOT,\n",
        "    \"u/N/in_tok\": TK_IN,\n",
        "    \"u/N/out_tok\" : TK_OUT,\n",
        "    \"u/N/cost\" : actual_cost,\n",
        "    \"rows_to_tree\" : wandb.Table(data=tl, columns = [\"comments\", \"taxonomy\", \"raw_llm_out\"])\n",
        "})\n"
      ],
      "metadata": {
        "id": "JILGhL2r5lNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: One comment > extract claims\n",
        "\n",
        "For each comment, extract claims and assign to a specific subtopic in the given taxonomy."
      ],
      "metadata": {
        "id": "Izyy-lyysAoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weave.init(WB_PROJECT_NAME)\n",
        "\n",
        "@weave.op()\n",
        "def comment_to_claims(comment:str)-> dict:\n",
        "    client = OpenAI()\n",
        "\n",
        "    # add taxonomy and comment to prompt template\n",
        "    full_prompt = COMMENT_TO_CLAIMS\n",
        "    taxonomy_string = json.dumps(taxonomy, indent=1)\n",
        "    full_prompt += \"\\n\" + taxonomy_string + \"\\nAnd then here is the comment:\\n\" + comment\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYS_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": full_prompt\n",
        "        }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "    claims = response.choices[0].message.content\n",
        "    return {\"claims\" : json.loads(claims), \"usage\" : response.usage}\n",
        "\n",
        "# track totals for this pipeline step only\n",
        "# what do we want to know for this step?\n",
        "# we want the individual counts of each\n",
        "TK_2_IN = 0\n",
        "TK_2_OUT = 0\n",
        "TK_2_TOT = 0\n",
        "\n",
        "c2c = []\n",
        "c2c_html = []\n",
        "with weave.attributes({\n",
        "    \"model\" : MODEL, \"stage\" : \"2_comment_to_claims\", \"run\" : RUN_NAME}):\n",
        "  for comment in comments:\n",
        "    resp = comment_to_claims(comment)\n",
        "    claims = resp[\"claims\"]\n",
        "    usage = resp[\"usage\"]\n",
        "    print(comment)\n",
        "    print(claims)\n",
        "    c2c.append(claims)\n",
        "\n",
        "    # format for logging to W&B\n",
        "    viz_claims = cute_print(claims)\n",
        "    c2c_html.append([comment, viz_claims, json.dumps(claims,indent=1)])\n",
        "\n",
        "    TK_2_IN += usage.prompt_tokens\n",
        "    TK_2_OUT += usage.completion_tokens\n",
        "    TK_2_TOT += usage.total_tokens\n",
        "\n",
        "    TK_TOT += usage.total_tokens\n",
        "    TK_IN += usage.prompt_tokens\n",
        "    TK_OUT += usage.completion_tokens\n",
        "\n",
        "\n",
        "    # update per-comment tokens\n",
        "    wandb.log({\n",
        "      \"u/2/s_N_tok\": usage.total_tokens,\n",
        "      \"u/2/s_in_tok\" : usage.prompt_tokens,\n",
        "      \"u/2/s_out_tok\": usage.completion_tokens,\n",
        "      \"u/2/t_N_tok\": TK_2_TOT,\n",
        "      \"u/2/t_in_tok\" : TK_2_IN,\n",
        "      \"u/2/t_out_tok\": TK_2_OUT\n",
        "    })\n",
        "\n",
        "# calculate and log actual cost here?\n",
        "actual_2_cost = (COST_IN_PER_10K * TK_2_IN + COST_OUT_PER_10K * TK_2_OUT) / 10000.0\n",
        "print(\"actual$:\", actual_2_cost)\n",
        "actual_cost += actual_2_cost\n",
        "\n",
        "wandb.log({\n",
        "    # update total token counts\n",
        "    \"u/N/N_tok\" : TK_TOT,\n",
        "    \"u/N/in_tok\": TK_IN,\n",
        "    \"u/N/out_tok\" : TK_OUT,\n",
        "    \"u/N/cost\" : actual_cost,\n",
        "    \"u/2/cost\" : actual_2_cost,\n",
        "    \"row_to_claims\" : wandb.Table(data=c2c_html, columns = [\"comments\", \"claims\", \"raw_llm_out\"])})\n",
        "\n"
      ],
      "metadata": {
        "id": "ETtcxo20uxFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: [nonLLM] Count + sort by claims DESC\n",
        "\n",
        "Sort the taxonomy by the number of claims in each subtopic, desc.\n",
        "Note that this pipeline stage doesn't call any LLMs."
      ],
      "metadata": {
        "id": "Ab-Blzxp2t9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TOPICS = 0\n",
        "NUM_CLAIMS = 0\n",
        "\n",
        "def sort_taxonomy(tree, c2c):\n",
        "  node_counts = {}\n",
        "  # count number of claims in each topic node of the outline\n",
        "  for cmt, cmt_claims in zip(comments, c2c):\n",
        "    for claim in cmt_claims[\"claims\"]:\n",
        "      if claim[\"topicName\"] in node_counts:\n",
        "        node_counts[claim[\"topicName\"]][\"total\"] += 1\n",
        "        if claim[\"subtopicName\"] in node_counts[claim[\"topicName\"]][\"subtopics\"]:\n",
        "          node_counts[claim[\"topicName\"]][\"subtopics\"][claim[\"subtopicName\"]][\"total\"] += 1\n",
        "          node_counts[claim[\"topicName\"]][\"subtopics\"][claim[\"subtopicName\"]][\"claims\"].append(claim[\"claim\"])\n",
        "        else:\n",
        "          node_counts[claim[\"topicName\"]][\"subtopics\"][claim[\"subtopicName\"]] = { \"total\" : 1, \"claims\" : [claim[\"claim\"]]}\n",
        "      else:\n",
        "        node_counts[claim[\"topicName\"]] = {\"total\" : 1, \"subtopics\" : {claim[\"subtopicName\"] : {\"total\" : 1, \"claims\" : [claim[\"claim\"]]}}}\n",
        "  return node_counts\n",
        "\n",
        "# log sorted taxonomy\n",
        "sorted_taxonomy = sort_taxonomy(taxonomy, c2c)\n",
        "print(sorted_taxonomy)\n",
        "html_data = [[cute_print(sorted_taxonomy), json.dumps(sorted_taxonomy, indent=1)]]\n",
        "\n",
        "for theme, topics in sorted_taxonomy.items():\n",
        "  for theme_key, topic_details in topics.items():\n",
        "    if theme_key == \"subtopics\":\n",
        "        for topic_key, claim_details in topic_details.items():\n",
        "            NUM_CLAIMS += claim_details[\"total\"]\n",
        "        NUM_TOPICS += len(topic_details)\n",
        "\n",
        "print(NUM_TOPICS)\n",
        "print(NUM_CLAIMS)\n",
        "\n",
        "wandb.log({\"sort_tree\" : wandb.Table(data=html_data, columns = [\"sorted_taxonomy\", \"raw_llm_output\"])})"
      ],
      "metadata": {
        "id": "mCHYoEpH29ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in sorted_taxonomy.items():\n",
        "  for i, j in v.items():\n",
        "    if i == \"subtopics\":\n",
        "      print(j.keys())"
      ],
      "metadata": {
        "id": "JCKplSYc5GDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Dedup claims in each subtopic\n",
        "\n",
        "Find similar claims in the list for each subtopic. This logic could be much cleaner."
      ],
      "metadata": {
        "id": "tcBsPcI_Csol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weave.init(WB_PROJECT_NAME)\n",
        "\n",
        "@weave.op()\n",
        "def dedup_claims(claims:str)-> dict:\n",
        "    client = OpenAI()\n",
        "\n",
        "    # add claims with enumerated ids\n",
        "    full_prompt = DEDUP_PROMPT\n",
        "    for i, rc in enumerate(claims):\n",
        "      full_prompt += \"\\nclaimId\"+str(i)+ \": \" + rc\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYS_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": full_prompt\n",
        "        }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "    deduped_claims = response.choices[0].message.content\n",
        "    return {\"dedup_claims\" : json.loads(deduped_claims), \"usage\" : response.usage}\n",
        "\n",
        "# we want the individual counts of each\n",
        "TK_4_IN = 0\n",
        "TK_4_OUT = 0\n",
        "TK_4_TOT = 0\n",
        "\n",
        "print(c2c)\n",
        "nested_claims = {}\n",
        "dupe_counts = {}\n",
        "with weave.attributes({\"model\" : MODEL, \"stage\" : \"4_dedup_claims\", \"run\" : RUN_NAME}):\n",
        "  tl_data = []\n",
        "  for topic, subt in sorted_taxonomy.items():\n",
        "    for sub_topic, subtd in subt[\"subtopics\"].items():\n",
        "      print(\"num claims: \", len(subtd[\"claims\"]))\n",
        "      # don't dedup solo claims\n",
        "      if len(subtd[\"claims\"]) > 1:\n",
        "        resp = dedup_claims(subtd[\"claims\"])\n",
        "        deduped_claims = resp[\"dedup_claims\"]\n",
        "        usage = resp[\"usage\"]\n",
        "\n",
        "        # let's check if they're duplicated?\n",
        "        # this is harder than we thought!\n",
        "        has_dupes = False\n",
        "        if \"nesting\" in deduped_claims:\n",
        "          for claim_key, claim_vals in deduped_claims[\"nesting\"].items():\n",
        "            if len(claim_vals) > 0:\n",
        "              has_dupes = True\n",
        "              # extract index...\n",
        "              ckey = int(claim_key[-1:])\n",
        "              dupe_keys = [int(c_key[-1:]) for c_key in claim_vals]\n",
        "              dupe_counts[subtd[\"claims\"][ckey]] = dupe_keys\n",
        "\n",
        "        # for logging to wandb\n",
        "        tl_data.append([\"\\n\".join(subtd[\"claims\"]), cute_print(deduped_claims), json.dumps(deduped_claims, indent=1)])\n",
        "\n",
        "        # append dupe claims & filter\n",
        "        if has_dupes:\n",
        "          nested_claims[sub_topic] = {\"dupes\" : deduped_claims, \"og\" : subtd[\"claims\"]}\n",
        "        wandb.log({\n",
        "            \"u/4/s_N_tok\": usage.total_tokens,\n",
        "            \"u/4/s_in_tok\" : usage.prompt_tokens,\n",
        "            \"u/4/s_out_tok\": usage.completion_tokens,\n",
        "            \"u/4/t_N_tok\": TK_4_TOT,\n",
        "            \"u/4/t_in_tok\" : TK_4_IN,\n",
        "            \"u/4/t_out_tok\": TK_4_OUT\n",
        "        })\n",
        "        TK_4_TOT += usage.total_tokens\n",
        "        TK_4_IN += usage.prompt_tokens\n",
        "        TK_4_OUT += usage.completion_tokens\n",
        "        TK_TOT += usage.total_tokens\n",
        "        TK_IN += usage.prompt_tokens\n",
        "        TK_OUT += usage.completion_tokens\n",
        "\n",
        "  # calculate and log actual cost here?\n",
        "  actual_4_cost = (COST_IN_PER_10K * TK_4_IN + COST_OUT_PER_10K * TK_4_OUT) / 10000.0\n",
        "  print(\"actual$:\", actual_4_cost)\n",
        "  actual_cost += actual_4_cost\n",
        "\n",
        "  wandb.log({\n",
        "      \"u/N/N_tok\" : TK_TOT,\n",
        "      \"u/N/in_tok\": TK_IN,\n",
        "      \"u/N/out_tok\" : TK_OUT,\n",
        "      \"u/4/cost\" : actual_4_cost,\n",
        "      \"u/N/cost\" : actual_cost,\n",
        "      \"dedup_subclaims\" : wandb.Table(data=tl_data, columns = [\"sub_claim_list\", \"deduped_claims\", \"raw_llm_output\"])})\n",
        "\n",
        "\n",
        "print(json.dumps(dupe_counts, indent=2))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-JocF-KDDrjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Save Approximate T3C Report\n",
        "\n",
        "Merge duplicate claims and log a simplified T3C report to W&B."
      ],
      "metadata": {
        "id": "HmF7k0ubVPah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synth_t3c_report(ttree, dupes):\n",
        "  ltree = {}\n",
        "  for theme, theme_d in ttree.items():\n",
        "    theme_total = 0\n",
        "    topic_list = {}\n",
        "    for topic, topic_d in theme_d[\"subtopics\"].items():\n",
        "      theme_total += topic_d[\"total\"]\n",
        "      if topic in nested_claims:\n",
        "        # this one has some dupes\n",
        "        # for each duplicate claim, we list the duplicate ids\n",
        "        # but we don't currently merge them as \"similar claims\"\n",
        "        rerank = {}\n",
        "        for c in topic_d[\"claims\"]:\n",
        "          if c in dupe_counts:\n",
        "            new_label = c + \" (\" + str(len(dupe_counts[c]) + 1) + \"x:\"\n",
        "            for ckey in dupe_counts[c]:\n",
        "              new_label += \" \" + str(ckey) + \",\"\n",
        "            new_label += \")\"\n",
        "            rerank[new_label] = len(dupe_counts[c])\n",
        "          else:\n",
        "            rerank[c] = 0\n",
        "        ranked = sorted(rerank.items(), key=lambda x: x[1], reverse=True)\n",
        "        new_claims = [r[0] for r in ranked]\n",
        "        print(new_claims)\n",
        "        topic_list[topic] = {\"total\" : topic_d[\"total\"], \"claims\" : new_claims}\n",
        "      else:\n",
        "        topic_list[topic] = {\"total\" : topic_d[\"total\"], \"claims\" : topic_d[\"claims\"]}\n",
        "\n",
        "    # sort topics\n",
        "    sorted_topics = sorted(topic_list.items(), key=lambda x: x[1][\"total\"], reverse=True)\n",
        "\n",
        "    ltree[theme] = {\"total\" : theme_total, \"topics\" : sorted_topics}\n",
        "\n",
        "  # sort full tree\n",
        "  sorted_tree = sorted(ltree.items(), key=lambda x: x[1][\"total\"], reverse=True)\n",
        "  return sorted_tree\n",
        "\n",
        "ltree = synth_t3c_report(sorted_taxonomy, dupe_counts)\n",
        "print(json.dumps(ltree, indent=4))\n",
        "\n",
        "# log sorted taxonomy\n",
        "html_data = [[cute_print(ltree), json.dumps(ltree, indent=1)]]\n",
        "\n",
        "# log final costs\n",
        "total_run_cost = TK_IN * (COST_IN_PER_10K/10000.0) + TK_OUT * (COST_OUT_PER_10K/10000.0)\n",
        "print(\"guessed: \", guess_cost)\n",
        "print(\"total from TK: \", total_run_cost)\n",
        "print(\"total from math: \", actual_cost)\n",
        "\n",
        "# TODO: print cost here!!!\n",
        "# log sheet\n",
        "#name,group,time,rows,chars,guess_cost,tok_cost,actual_cost,1_cost,2_cost,4_cost,num_themes,num_topics,num_claims\n",
        "#eventually we want these more..granular/averaged?\n",
        "\n",
        "log_row = [RUN_NAME, EXP_GROUP, time_here(),len(comments),comments_total,round(guess_cost,2),\n",
        "           round(total_run_cost, 2),round(actual_cost,2),round(actual_1_cost,2),round(actual_2_cost,2),round(actual_4_cost,2),\n",
        "           NUM_THEMES, NUM_TOPICS, NUM_CLAIMS ]\n",
        "csv_log = \",\".join([str(x) for x in log_row])\n",
        "print(csv_log)\n",
        "\n",
        "wandb.log({\n",
        "    \"cost/tok_total\" :  total_run_cost,\n",
        "    \"cost/actual\" : actual_cost,\n",
        "    \"cost/1\": actual_1_cost,\n",
        "    \"cost/2\" : actual_2_cost,\n",
        "    \"cost/4\" : actual_4_cost,\n",
        "    \"csv_log\" : csv_log,\n",
        "    \"t3c_report\" : wandb.Table(data=html_data, columns = [\"t3c_report\", \"raw_llm_output\"])})\n",
        "\n",
        "\n",
        "# done with this run!\n",
        "wandb.run.finish()"
      ],
      "metadata": {
        "id": "jleyPa2AXNOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "## Dev TODOs\n",
        "- log raw dictionaries as well as nicely-formatted json so we can download & post-process later\n",
        "- we need to sum the stage 2 token counts\n",
        "\n",
        "- we need to count / estimate / track tree size\n",
        "\n",
        "\n",
        "- condense prompts\n",
        "- enable cost logging\n",
        "- enable different models\n",
        "\n",
        "- compare swapping long/short description names in prompt\n",
        "- add unit tests based on comments\n",
        "- double-check/improve model config logging in W&B runs\n",
        "- check: allow for empty comments, w/o meaningful claims\n",
        "- track finish_reason = completion.choices[0].finish_reason\n",
        "\n"
      ],
      "metadata": {
        "id": "0jqWq_ZJtDPB"
      }
    }
  ]
}