id,comment
1639983860166414336,"We still have no good candidate for decision theory:

""Advocates of Causal Decision Theory (CDT) argue that Evidential Decision Theory (EDT) is inadequate because it gives the wrong result in Newcomb problems. Egan (2007) provides a recipe for converting https://t.co/BqvJqJrhMt‚Ä¶ https://t.co/ALUT3rLVOe"
1638992216755679232,"If widely shared views change randomly over time, we should see ""progress"" in the past toward current views, but expect anti-progress in the future, relative to our current views. https://t.co/D4lZ6hXAQh"
1638979989461962754,"""We could design an AI bill of rights not because the AIs today need one but because, by the time they do need one, it may be too late.""

I'm open to considering such a thing, but I do need to see a specific proposal. https://t.co/xIY43lp7if"
1638508020773670913,"Uh oh: ‚ÄúAs of April 2022, about one-third of U.S. states had proposed or enacted at least one law to protect consumers from AI-related harm or overreach.‚Äù https://t.co/ks5S8FD9fr"
1638314353660248066,"@RokoMijic @BasedBeffJezos And yet, no one should have tried to slow down the adoption of firms due to alignment concerns."
1637422546613149700,"‚Äúsolv[ing] the rebooting problem .. will likely require a more radical shift in design, perhaps even a wholesale abandonment of the transformer architecture on which every GPT model has been built.‚Äù"
1637422187278729218,"‚ÄúGPT4 .. can take into account only a limited amount of text, .. short enough to render all sorts of complex tasks impossible.‚Äù https://t.co/BDjBAct6RM"
1637178923199987712,This seems good. Even better would be to open-source the criteria used to design and pick such code. https://t.co/jGSFXlfsGC
1637171125565235200,"It seem sufficient that AI be law abiding. Most likely from its owners being such, &amp; liable for AI harms. AI doesn't need to love us more than that. https://t.co/J4qA5tIlhq"
1643383389351981056,it's essential to revisit these critical perspectives to ensure a comprehensive understanding of the situation and inform future decision-making https://t.co/PB5tqXdoRE
1643073662571130882,"8/ üì¢ No one's questioning it... Maybe we should make sure this never happens again. ü§îüîä
Well - last week - we tried. Stay tuned.
 https://t.co/WXvnCyye8y"
1642864754715934721,100% true. That‚Äôs why we did this! https://t.co/78cZeIpTL2 https://t.co/kT2hj52HHW
1642677764699615233,üëá https://t.co/Dq8uTqIjJl
1642000392631504896,This is the most important issue of our day. https://t.co/Q3d4hnJ3yW
1639248195527032832,"You have 2 actions:
1) Get the book: https://t.co/UiRuU87oqI
2) Read the article: https://t.co/El2jRgJQi2"
1638716824262156290,@ShaulaetLesath Someone shared this with me. I think it timely. Meant no offense to the dead but these issues are still out there - prominently.
1643611381977997313,"Was literally Just telling my students about how gpt &amp; other ML tools are giant confirmation bias engines, &amp; then bloomberg goes &amp; slaps one in an environment as influencable as capitalist finance. Just tip tops. Gonna be an overdetermined feedback reinforcement looping nightmare"
1641609676432326657,"I said, ""YOU KNOW THE MAIN SIGNATORIES AND PROPONENTS OF THAT OPEN LETTER CALLING FOR A PAUSE IN 'A.I.' DEVELOPMENT DON'T ACTUALLY CARE ABOUT THE SOCIOETHICAL IMPLICATIONS OF 'A.I.' AND  JUST WANT TIME TO SURREPTITIOUSLY ADVANCE THEIR MODELS AND SHAPE ANY NEW REGULATIONS, RIGHT?"""
1641116879329583104,"Literally all vN meant was a time when the vast &amp; complex computing systems interwoven w/ our lives would develop at a pace beyond our understanding &amp; control. He used ""singularity"" advisedly: Beyond the event horizon, where the gravity becomes inescapable. The point of no return"
1640940603373494272,"@brodiegal Skynet did nothing it wasn't built, programmed, and trained to do ü§∑üèø‚Äç‚ôÇÔ∏èü§∑üèø‚Äç‚ôÇÔ∏è"
1640037563825045506,"And that human behavior of heartless automatic medical insurance denial is being used as training data for automated algorithmic ""A.I."" systems which will identify, capitalize on, exacerbate, and iterate from the patterns it finds.

It's bad! üòÉ https://t.co/RT6TMlYD4M"
1639639109143609346,"Would really like @OpenAI to be meaningfully transparent about whether and exactly how they plan to be responsive to those experts who point out potential problem areas in openai's operations and structuring; how are you ""aligning"" your systems &amp; values, or moderating content?‚Ä¶"
1638730150131924992,Activists literally warned you that exactly this kind of thing was going to happen https://t.co/aFkrrhr4v6
1637101488563449856,"Epistemic crisis go brrrrrrrrrrrr
https://t.co/xswGfC3j3g"
1640377375106924544,"@ChrisMurphyCT Hello senator, I am an AI practitioner with a PhD in computational chemistry. I run a small business consultancy that advises on AI governance. People like us are available to cut through the hype and provide informed opinions on what's really going on in this field."
1638197841108652034,"The field of responsible AI / AI ethics / AI safety / AI alignment has some serious growing up to do

Armchair trolley problem ethicists need to understand that in the real world, people still need to ride and drive the damn trolley. Capitalists ignore ethics without pragmatics https://t.co/x5bmVWSmpN"
1637467440023044099,"How to mislead LLMs like @Anthropic's Claude+:

Generate some very plausible code and do something just offbeat like omitting a plot function.

Watch as the LLM hallucinates what the code does based on thousands of tutorials it has ingested, and makes wrong claims! https://t.co/Az7wqxga7z"
1638727820112449536,"Bluh. I should have expected this. Had a vague premonition but it wasn't enough to change my behavior.

OK: I commit to no more public blather about ""cool things in AGI-land"" until I can more clearly articulate  my moral responsibility here. https://t.co/QsJZ2M10tX"
1638291640115929088,"@nickarner Others have made this case much better than I can in this little box.

In brief: we seem to be increasing our probability of catastrophe much more rapidly than we're increasing our probability of robustness, and the second derivatives make that situation look likely to get worse."
1638290725099167752,"But designing an interface which incorporates GPT-4 and by doing so makes it utterly indispensable in a particular context? I'm conflicted. My moral instinct leans negative, with wide confidence bars. I'm worried that my creative instinct is trying to get me to ignore that."
1638288801083842573,"Does designing powerful new systems that leverage highly general models‚Äîeven innocuously‚Äîconstitute a (small) contribution to AI capacity? (In some cases, I think so.) If so, where do I place my moral boundaries? Feeling awfully confused."
1638963983045783552,We just announced ChatGPT plugins ‚Äì¬†which unlocks the ability for the model to use tools in a safe way. Pretty crazy what you can do with it already with the initial set of plugins! https://t.co/O4Vy4WiizA
1643710672746987520,"Yup, the most urgent alignment problem is that of aligning the well being of the planet and its inhabitants, with the actions of the companies and governments running it. 
https://t.co/xkM7ISbpZa"
1643699387053531136,"Many ppl who are not so close to AI, ask me if I'm worried about x-risk.

Truth is, yes I believe there is a possible future where a very powerful AI causes catastrophe. But I don't believe this is inevitable. I believe we would have to be incredibly stupid to get to the stage‚Ä¶ https://t.co/0gJP6s64fo"
1641503381083820032,"It is important to dismantle any myths that the way these systems operate are anything remotely like that of the human mind. But to push narratives that ""intelligence"", ""creativity"", ""understanding"" are uniquely human abilities, just because ü§∑‚Äç‚ôÇÔ∏è, are imho equally irresponsible -‚Ä¶ https://t.co/zQGzGoUFVf"
1640951318234955777,"it's wild seeing the response from the people who have been trying to slow down AI for years, to the petition trying to slow down AI, because even though the petition trying to slow down AI lists the same reasons as the people who have been trying to slow down AI for years, they‚Ä¶ https://t.co/ZDEP4Qfals"
1640061012073406465,"üßµ It's so fascinating that it has access to all the info it needs to evaluate the correct answer (too much info in fact üòÖ), but it isn't always able to select and give attention to all of the essential bits of info. A little bit of coaxing from me however, primes it‚Ä¶ https://t.co/BNVf0YTZDi https://t.co/sIKHRRIjAk"
1639923979753517058,"@ESYudkowsky here is a thread
https://t.co/LOgiCL0QRX"
1638309526175907840,"There's discussions going around with regards to whether or not Google's Bard is trained on gmail. 

Many people are rightly concerned about privacy. If everyone's gmail is used for training the publicly accessible Bard model, that is of course a complete privacy disaster.

I‚Ä¶ https://t.co/ZnUJdlUYT7 https://t.co/a34Kpz5mP7"
1636641698330681344,"Isaac Asimov's ""Three Laws of Robotics"" are very anthropocentric.  There is no rule protecting robots from humans.

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

2. A robot must obey orders given it by human beings except‚Ä¶ https://t.co/j3r4CdPIln"
1636443030512123928,"GPT-4 is here. And it's powerful. In this thread I will collect my observations before I compile them to an essay.

/Thread https://t.co/Cv1rhiqAB7"
1643769291928641537,something something gnome argument https://t.co/72dFw7bbE5
1643655501841063937,"argument for why to keep going forward with ai: if china came out with chatgpt, yuddite would be arguing for nuking them now, whereas today the bar is gpt5 or greater! https://t.co/532fOFpA6f"
1643437607098155008,@passionfingerz @shorttimelines @ESYudkowsky here are my results: https://t.co/cJzPQoDzUL
1643087560124817408,"People are not talking enough about the benefits to the open, empirical approach to AI alignment research https://t.co/oxDHc1A02b"
1643074329972989952,thoughts on ai safety https://t.co/WWzBVeYNoF
1641602017058779136,"don‚Äôt worry officer this intelligence is only trained on decentralized, organic, free range gpus,

all our parameters are grass touched"
1641361934699356161,"people are saying that the AI doomerism isn't aligning along traditional ideological tribes, but, that's not quite true.

it is definitely a thing where it is wordcels on one side, and shape rotators on the other"
1641355318537105409,"i was in the e/acc space tonight and somebody asked ‚Äúwhy is the return to investors, capped?‚Äù with regards to OpenAI

the hosts, stumped, couldn‚Äôt come up with anything better than virtue signaling.

ridiculous! i will tell you why. OpenAI genuinely believes in the upside of‚Ä¶ https://t.co/HNlZRXmGCd"
1641315972861808645,"Stoke's Law: ""as an online discussion of AI safety grows longer, the probability of a comparison to nuclear weapons approaches 1."" (ht @jonst0kes ofc)

https://t.co/EHGVES8owI https://t.co/6JT3CRI790 https://t.co/43sPrZd6Jf"
1641245591513743360,find myself nodding to this https://t.co/WSxXJ5zOq3 https://t.co/BuGctFrm5H
1640090570726178817,"of course people are doing this, and tbh i have been on the side of ""let's try things and see what happens,"" but, I do think we're going to have to be pretty productive in terms of spinning up AI antibodies

https://t.co/dQMNQKUQaQ https://t.co/3joshkPFyk"
1639424642816577536,who could have foreseen correlated disasters? https://t.co/tHAzWaznuF
1638990572395589635,pandora‚Äôs wager
1638412095661412352,"@APompliano @balajis this is a tour du force by @balajis; while i would certain remain on the counter party side of his bet, i‚Äôm rather in agreement with the analysis he puts forward, great interview https://t.co/J338XJwHNA"
1637177360243113984,@elonmusk @ESYudkowsky https://t.co/2X0PfRE0y5
1643657666873671680,One thing the doomers fail to consider is that the *difficulty* of making further improvements hits a vertical asymptote as you approach fundamental limits. So we need ASI just to keep things improving at a *moderate* rate. There‚Äôs no danger that rates of improvement go vertical.
1643315924404146176,"@jradoff They literally have an amateurish, far too simplistic view of it!! The practical limits are much closer than they realize, and the engineering issues of adiabatic/reversible are far more difficult than they realize! I know b/c I‚Äôve spent my career working on this, not blogging!"
1643306945573720064,"I remember when people were speculating that Dall-E had its own internal language that explained some of the weird text that it wrote.  Well, GPT-4 definitely does, it looks like. https://t.co/MqxSIaoRpE"
1643273846831878144,"What‚Äôs especially hilarious is that these people think an intelligence explosion is possible when we‚Äôre already starting to grind up against the thermal noise limit, and literally almost nobody is investing in the only concept (reversible computing) that might let us get past it."
1642565407423987712,"@IntuitMachine This is exactly what OpenAI is doing. Since they haven‚Äôt released their base model of GPT-4, but only the RLHF‚Äôd version (masked Shoggoth), there‚Äôs no way for the world to know its true capabilities."
1642177627300691968,AGI seems about to bust outta the chest of the open-source community https://t.co/DzZjjfEEnd
1642036802814111744,"@IntuitMachine @Slplss_In_Japan Imo we definitely need these things to have more agency, so they can tackle more complex tasks effectively."
1642032983485071360,This looks important https://t.co/8CTjONBA6x
1641936309928464385,"Yet, the few people in the world who are seriously studying this topic struggle to get funding. Why is this?"
1641936044315770881,It‚Äôs incredible to me that a larger amount of brainpower isn‚Äôt dedicated to understanding the physical limits of computing. The question of how great intelligence efficiency and density can get critically depends on this.
1641116658260492290,"I think a much better idea would be to call for open-sourcing of all AI tech. The biggest danger here is the asymmetrical advantage of leading firms, and the possibility that this could lead to overwhelming dominance and an AGI singleton that could quite easily become corrupted. https://t.co/Yv5UX1ZJsP"
1640901723119144963,Interesting predictions from an OpenAI researcher https://t.co/Vgo2RwmF6X
1639856880708366336,"@sama - This was a good interview, but I have to say, pls release the base GPT-4 to the research community (at least!) sooner rather than later!! Even if it is hard to use, if thousands more people poke at it, they are sure to discover very important new things to know about it!"
1639234909003984897,"I want an AI research tool where I can just say to it, ‚ÄúPlease gather, read and summarize all of the papers you can find on topic X, index them via embeddings, summarize the summaries hierarchically to condense them and then write an up-to-date review of the state of the field.‚Äù"
1638883484335841280,So wait I could build a system capable of running a model supposedly rivaling PaLM (8bit LLaMA 65B) for less than the cost of a car? ü§î https://t.co/wFPgjerUUO
1638610252487888896,"@harmlessai Yeah imo all the fearmongering about ‚Äúsafety‚Äù by leading AI firms is ultimately just a cynical excuse to build moats. Regulatory capture / crony capitalism is the end goal. Even if not consciously, that‚Äôs what will end up happening."
1642982815989112832,"GPT-4 lovers: SudoLang v1.0.5 is here.

SudoLang is a powerful natural-language constraint-based pseudocode programming language for GPT-4+ that give your GPT-4 prompts superpowers.

https://t.co/KcNM8kq6TZ"
1642802032296992768,3) We need to articulate the intersection of abstract AGI theory with everyday human life and human-world practical tasks with sufficient clarity that only a tiny minority of AI experts will be confused enough to answer a question like this with YES ...
1642802029071601665,"1)  Well holy fukkazoly, I have to say I'm surprised  by these results.   Yah it's unscientific but ... perplexed that half my self-described AI expert followers think GPT-n systems w/o big additions could yield HLAGI.   OMG.  Nooo wayyy.  Theory of HLAGI urgently needed https://t.co/IDuQmvnM46"
1642323932153004032,David Brin's very clear minded view on the recent absurdist petition to pause work on GPT-5 and similar... https://t.co/RfoGfKKn2b
1642032533943758850,@primalpoly @archerships @GaryMarcus Yes we should invest more in longevity research.   However it's pretty clear that a human-level AGI connected directly w/ ML and bioinformatics software and robotized lab equipment  -- and guided by human experts at the start -- could cure aging far faster than human scientists.
1642030348040273921,"@GaryMarcus @ESYudkowsky The US pausing all AI R&amp;D for 6 months would not give China enough time to catch up in AI.   However, I can't see what would happen in 6 months (esp. w/o AI R&amp;D) that would make the FLI and their chums OK with restarting fast-paced advanced AI research 6 months later."
1641986337879511040,"Will GPT-n systems (or similar), w/o addition of major new ideas/components not used in GPT-4, ever reach the general intelligence, logic and creativity level of top human scientists, artists and leaders?"
1641498066904842241,"@bramiozo Upside of beneficial AGI is obvious; as IJ Good said in 1965 ""the first ultraintelligent machine is the last invention humanity needs to make"".    @sama and I seem to agree here   Cure aging/death &amp; mental illness, make strong nanotech work, mind-uploading, cheap food/energy etc."
1641219163023872000,"14) The correct reaction to Big Tech making waves with centralized data-bound fake-AGI systems is not to make Big Tech pause, but rather to take up the challenge and move FASTER than Big Tech toward democratic/decentralized/beneficial real AGI systems @SingularityNET"
1641219158498222081,"13) We should be proactionary not precautionary, and we should couple this proaction with a focus on open, democratic and decentralized systems that are applied to applications of broad human benefit."
1641219154152927232,"12) We are at the eve of the Singularity, folks.   Indeed we don't want to be eaten by malevolent AGI or turned into gray goo.  But the tech that leads us to Singularity is going to be weird and complex with hard-to-analyze pluses and minuses all along the way."
1641219145080664064,"10) What might go wrong: China overtakes the US in big-LLM development (because they are not slowing down, and are not shy about applying their best AI tech to military and espionage)"
1641219137363140608,8) So what would go wrong if the big Western tech companies actually paused big-LLM R&amp;D ?  Let's see...
1641219133521166337,7) But you don't get all fearful and pause exciting new technologies with complex mixes of good and bad aspects and no immediate huge deadly threat associated with them.
1641219129645633537,"6) Similarly yeah you take action against an LLM that demonstrably turns everyone who talks to it into murderous psychopaths, or a 3D printer whose express purpose is to cheaply 3D print bombs out of ordinary kitchen ingredients..."
1641217397901369344,"1) A bit more on this call for a ""pause"" in development of big LLMs ....  BTW I know quite a lot of AI experts who are opposed to the notion but see no upside in projecting their views publicly...."
1640975832616427526,"@GaryMarcus @elonmusk @tegmark @Grady_Booch @AndrewYang @tristanharris We should be massively funding open decentralized multi-paradigm proto-AGI (including but not limited to LLMs), not slowing down ... Beneficial AGI is humanity's best hope and in spite of its corporate closed narrow aspects GPT-4 is helping us push there"
1640973993657368582,"@GaryMarcus @elonmusk @tegmark @Grady_Booch @AndrewYang @tristanharris To be clear, I do think there are some situations where  AGI or even narrow AI dev would best be paused while safety considerations are pursued.   But I don't feel this is such a case, I would want to see a far stronger argument than seems to exist here..."
1640959985692835840,"@GaryMarcus @elonmusk @tegmark @Grady_Booch @AndrewYang @tristanharris 3) Note, I am a major AI optimist and I believe AGIs (with LLMs as a component) are likely to bring an era of glorious abundance, but I do worry about chaos and suffering in the developing world in the transitional period. Don't see how a pause in LLM training will help w/ this."
1640959201584517120,"@GaryMarcus @elonmusk @tegmark @Grady_Booch @AndrewYang @tristanharris 1) The real human damage most likely to happen as LLMs advance and then AGIs start to take their place is in the developing world, where UBI for folks whose jobs are eliminated by AI will be harder to come by.   Better to have a petition for global UBI as a preventative measure."
1640958289092677633,"@GaryMarcus @elonmusk @tegmark @Grady_Booch @AndrewYang @tristanharris More powerful LLMs will be more useful as components of multi-paradigm AGI systems, which if built with compassion and wisdom and proper cognitive architecture give humanity its best hope of finding an amazing positive future."
1640496903136882689,"9) .. and compassion also must be instilled in the initial learning/experience of the AGI system, via having the system grow up doing things involving I-Thou interactions and giving lots of opportunities to exercise compassion..."
1640496901333323777,"8) I don't think super-compassion is automatically going to be there in any HLAGI or super-human AGI architecture though.  I think it needs to be instilled in the architecture of the system, not only in the goal system but in the initial assemblage of cognitive components ..."
1640496899777265664,7) By creating AGI systems with ability &amp; inclination to simulate other minds and analogize between properties of other minds and themselves -- we can create powerful empathy machines with many fewer perversities and limitations to their empathy than we see among humans....
1640496898082762752,"6) I believe we can engineer machines that are supercompassionate as well as superintelilgent relative to humans, and that they don't need to come by their compassion the same way that humans do..."
1640496896224665605,"5)  However I think/feel that compassion and empathy are reflective of universal aspects of mind, universe and existence ... and can be manifested in engineered AGI systems in ways that are different from  the way they manifest themselves in humans and other biological organisms"
1640496892495945728,"3) It's true that humans come by their compassion and empathy in routes deeply rooted in their biology, which is a route that will be hard for engineered AGIs built on anything resembling current hardware fabrics.."
1640496891128602624,"2) It's a mistake to over-anthropomorphize or over-biomorphize AGIs, and also a mistake to think about AGIs too closely by reference to current deployed commercial AI systems ....  Self-organized minds seeded by engineered AGI systems will be quite different"
1640496889283084288,"1) Compassion of (maybe near) future AGIs toward humans: This is certainly an area our science does not yet cover effectively, so we are all going in some measure on intuition.  So let me share some of mine..."
1639513739824336896,"@elonmusk Fortunately to get real human-level AGI one will need strong subsystems for advanced reasoning &amp; evolutionary creativity integrated with LLMs, &amp; while OpenAI is not going there, some of us more dedicated to benevolence and decentralization totally ARE @SingularityNET  @True_AGI"
1639380178521718786,"@WillyS0L Ability to make creative leaps beyond its training data, and then assess which of these leaps seems most likely valuable toward given goals.  Doing this clearly and plainly but unsystematically would be a real""spark of AGI.""  We'll be there soon."
1639379640568647680,"The  main issue GPT4's ""allegedly AGI-ish"" properties raises: If this sort of fairly funky emergence comes from scaling up a ""transformer NN ++"", what kind of amazing wild-ass emergence will we see when we scale up AI architectures w/ more recurrence, reflection and abstraction?"
1639378920465395712,"I don't think GPT4 shows ""sparks of AGI"" in a very useful sense (though given the lack of agreed definition of AGI it's not a totally insane statement).   I do think it shows interesting aspects of emergence, which did not emerge in similar systems at smaller scale.  It's cool."
1639378492562489344,"@Trrstnn Non-AGI systems can possibly obsolete 80% of human jobs, and do tremendous good or harm for the world.  However they cannot on their own lead to an Intelligence Explosion ... to get there we need systems smart enough to do highly-original cutting-edge engineering &amp; science"
1639170933536747526,"Looking at how GPT4 works, you'd be crazy to think it could be taught or improved or extended to be a true human level AGI.   Looking at what it can do, you'd be crazy not to see that w some human creativity it's got to be usable to greatly accelerate progress to true HLAGI ...."
1638901414108835840,"And , the fact that we currently  lack the right language to concisely, compellingly and nontechnically describe the basic limitations of LLM type architectures, does not make them any less real... Our languages did not evolve to discuss such creations!"
1638900793532137472,"I have the same sense, however strong limitations in terms of 1) powerfully inventive creativity, 2) dealing with highly novel situations, 3) sustained chains of careful original reasoning, will remain for LLM type architectures https://t.co/zWXnOpyi7n"
1638678451962912769,"But don't worry too much -- just enough.   LLMs ain't AGI and can't be upgraded into AGI, though they can be components of AGI systems with real cognitive architectures and reasoning/grounding ability.   What's key is that when actual AGIs are rolled out they're doing good works."
1638678447504642048,"I used to say the main commercial apps of AI were Selling, Killing, Spying and Crooked Gambling (aka Wall St.)  (not necc. in that order).   Now I'll add Bullshitting and Hallucinating to the list, thx OpenAI!   Thus we shape the first AGI minds, what could possibly go wrong?"
1638615510991470592,"And when we do roll out Decentralized LLMs, versions that use neural-symbolic reasoning to ground statements and respect truth (in its various subtle dimensions) will not be too far behind ;) https://t.co/sSLPeCVP4W"
1637487302346788865,22) .. OK OK even perhaps some sort of formal neural nets concoction with way more architectural richness and recurrence than GPTn and some cleverer learning algo than backprop.   OpenCog is just the route I best understand...
1637487285049368577,"20) -- oh yeah, and we need to ensure these AGIs don't fall under the control of centralized authorities with narrow self-centered goals, like national governments or large corporations -- cf @SingularityNET @nunet_global  @Hypercycle_AI etc. etc."
1637487279689154562,"19) And yes of course the AGIs must build and grow and evolve their own truths open-endedly, but let's have them do this based on a rich intuitive feel for human truth in its various wild dimensions"
1637487273980641281,"18) We get to AGIs that understand deeper human truths by building AGI systems that embrace the breadth and richness of human intelligence, and raising their minds up in a variety of mind and heart building social situations, including those involving doing good works --"
1637487256536592392,16)  AGIs in human-like robots and avatars entering into various meaningful situations will come to understand what it is to be a human world and mind and heart building with other humans.
1637487247820767237,15) So we need a cognitive architecture that is richly capable of compassion and cooperation.  Check.  Simulation and inference are the key aspects of theory of mind.
1637487243119017985,"14) What we want is for it to enter into compassionate collaboratory relationships with humans, so as human-level AGI is achieved and passed humans can open-endedly evolve in sync with human-level subnetworks of the super-AGI mind."
1637487235061694465,13) But what's the guarantee this system will stick to its initial goals as it evolves?  On the contrary we have a near-guarantee it will not.
1637487227172208643,"12) The evolutionary self-organization not the goals are the top level.  Open-ended intelligence FTW. We need a cognitive architecture that supports flexible goal-pursuit but doesn't deify it, allows it to live and flourish in a broader context."
1637487218645139456,"11) But wait, don't we need goals and rewards?  Yes, sort of.   A human-level human-like mind crystallizes its own goals as it develops, in resonance with its environment and the other agents therein, and abandons and/or morphs these goals as it grows."
1637487198839664643,"8) To do fitness evaluation for evolutionary learning in a practical way we need world-simulations.  Wait, we need these for inference too, for world-modeling, which GPT4 still mostly sucks at, as will GPTn."
1637487178014941186,"5) Yes, it's possible to do evaluation of soundness/validity/quality using neural methods, but this route is not nearly so clear as the route of putting symbolic inference engines in this role, and there's no reason to believe NNs will outperform symbolic systems here"
1637487169127211010,"4) To evaluate LLM hypotheses for validity, LLM arguments for soundness, or LLM artworks for quality, requires something quite different -- probabilistic/fuzzy symbolic inference engines provide one candidate system-type that can do this sort of thing"
1637351893637668864,"4) Weaver notes that stablecoins achieve stability only via centralization and reliance on fiat reserves -- yes, but Cogito solves this, with tracercoins that are stabler than fiat and don't need banks. @CogitoProtocol"
1637351888768122880,"3) Weaver notes the cost and inefficiency of blockchain networks -- yes but Hypercycle's ledgerless design, TODA + proof of reputation, solves this @Hypercycle_AI"
1637008336691531776,"@primalpoly @AngeloDalli @GaryMarcus ""Alignment"" is neither well posed nor solvable.  Humanity is not aligned with itself and this is a feature not a bug.  We must go full speed ahead on creating loving compassionate AGI before humanity wrecks itself and the planet with other tech and general foolishness/perversity."
1640263474935726081,@arul_1111i It‚Äôs worth noting that a lot of this (needing to focus on devs etc) was stuff many of us tried to push for years (i have a comprehensive 15page doc on how to bring in developers from early 2020 I wrote) and we were all told to sit down and focus on cbdc/drones/ev races etc
1643815259466256385,"Imagine VideoGPT, a future version of ChatGPT that generates scripted video in response to human prompts and allows video input. 

Things get weird: 

- Learn anything
- Custom movies made for you
- Talk to VideoGPT on Zoom
- VideoGPT joins Zoom as you
- VideoGPT as babysitter"
1643680688003239936,"This thread is fascinating. LLMs with RLHF are incredibly effective problem solvers we can assign tasks to. From a practical perspective, if most humans can‚Äôt tell whether GPT-4 is memorizing vs. reasoning, does the distinction even matter? https://t.co/TqpGT005t3"
1643463805433974785,"If you still think ChatGPT is just AI hype, try to pay attention to what‚Äôs happening when people approach GPT-4 with an open mind. 

Here‚Äôs an example from today: https://t.co/F96niBsegW"
1643391320361402369,"@OfficialLoganK Maybe @OpenAI should probably have a ‚Äúprompt defaults‚Äù settings page where customers can dial up or down various settings like this.  For now, you can just ask it in the initial prompt to not to bother with extraneous filler text like that when you start a new session."
1642777609426853888,This LLM survey paper is worth reading: https://t.co/7SDx1pAvQb https://t.co/adKzFmemdF
1642721632144990210,"ü§ØNew paper: ‚ÄúSelf-Refine: Iterative Refinement with Self-Feedback‚Äù shows LLMs can improve themselves without humans.

‚ÄúSELF-REFINE is unique in that it operates within a single LLM, requiring neither additional training data nor reinforcement learning.‚Äù https://t.co/8Coo3T66kZ https://t.co/ubNWDCQHS9"
1642681106532294658,"If you prompt GPT-4 to write some code, the response often leads to a multi-prompt conversation where you ask it to debug errors or make improvements until the code works as desired. Now imagine removing yourself from the loop by asking GPT-4 to talk to itself internally instead. https://t.co/dhHCUceswf https://t.co/b9ukcQT9V1"
1642277389987352576,"‚ÄúReason is only as good as the information we give it‚Äù

We will see many highly specialized knowledge bases plugged into GPT-4, increasing the sophistication and accuracy of answers. The constraints of factual grounding won‚Äôt reduce creative responses, but instead improve them. https://t.co/96HM8Cy6vr https://t.co/aFkXCvXhL2"
1642109341502435332,"This is similar to what humans do spontaneously when reading, thinking about a topic, or making a decision. We ask ourselves questions, generate likely answers, predict consequences, evaluate and debate internally, then generate more questions in an iterative fan out/in process. https://t.co/BrK7253Hq4"
1642102273462652929,Don‚Äôt assume GPT-4 can‚Äôt do something. Try giving it more challenging requests. Listen to any objections it makes and help it find ways to work around them: https://t.co/7xMmbGrRui
1641980648356454402,"If I‚Äôm experiencing this, imagine what it feels like to be working on GPT-6 at @OpenAI"
1641356685121032198,"In this thread, Jim asks GPT-4 to infer the cause of its ability to infer causes. GPT-4 responds with a 5-bullet *self-reflection* describing a plausible explanation. https://t.co/k6ZZ7245y6"
1640821207070277632,"@nirsd I‚Äôm picturing a default GPT-4 setting that translates any prompt you type into higher IQ language, similar to a ‚Äúturbo button‚Äù on a video game controller. Alternatively, it could ask follow up questions: ‚ÄúI could answer that, but what is the real problem you are trying to solve?‚Äù"
1640759969397215232,I‚Äôve talked to otherwise smart people who dismiss GPT-4 because they tried it briefly with vague prompts and it gave them uninteresting responses. They appear to be asking the wrong questions.
1640758062016507905,"GPT-4 appears to do a kind of ‚Äúsocial mirroring‚Äù. If you give it an intelligent question about an unsolved problem a PhD in computational neuroscience might ask, you get a more intelligent answer. A similar question in the style of a 5th grader gets a 5th grade level answer."
1640754968654385152,There‚Äôs probably a deeper cost to lobotomizing a LLM like this. Having an expansive world-model that includes seemingly unrelated domains of knowledge likely helps GPT-4 ‚Äúconnect the dots‚Äù and reason in unexpected ways that humans haven‚Äôt considered. https://t.co/umGnSDJLID
1640440432902692864,"Feeling a combination of anxiety, excitement, and exhaustion lately as you hear about GPT-4?

That‚Äôs probably your subconscious working overtime to update your own world model. The landscape changed, and an avalanche of new opportunities and potential dangers from AGI is near. https://t.co/hQL3F0M3pR"
1640171946968231936,I‚Äôm surprised how many people are not paying attention to what‚Äôs happening right now with GPT-4 https://t.co/lDxS34Naie
1639920213595684865,Things will get weird when everyone starts using ChatGPT or other LLMs both to write their emails and to think up responses to send. GPT-4‚Äôs email replies will essentially be predictions based on predictions of its likely own responses. That‚Äôs mind reading.
1639901202673725444,"So many implications and likely consequences of GPT-4 running through my head right now.

Here‚Äôs one: telepathy. üò≥"
1639894112307679232,Stop what you‚Äôre doing and read this entire post on the GPT-4 code interpreter plugin. This is completely bonkers and is going to change everything. https://t.co/0VHHb47dAQ
1639841969487888384,‚Äúmodel vampire attack‚Äù has now entered the AI lexicon https://t.co/thWZ7Keqf2
1639066330300284928,GPT-4 is one of those rare ‚Äúdrop everything‚Äù moments where science suddenly takes a giant leap forward. I don‚Äôt think most people outside of machine learning understand how significant this is. https://t.co/UQDglUBPrz
1638358062993149954,Great to see people move beyond static QA and experiment with interactive problem solving like this: https://t.co/PuazG4Jb2q
1637893075673706497,"If you think things are wild right now, imagine what will happen when GPT-4 style models are trained on the entire corpus of the world‚Äôs social media posts from Facebook, Instagram, TikTok, and other social media."
1637296776939200512,@awadallah @OpenAI You didn‚Äôt say ‚Äúonly‚Äù? it‚Äôs not the best at inferring intent or satisfying implied constraints when asked unusual questions.
1637277965192134658,"Apps powered by GPT-4 like models that run entirely on your local laptop, fine-tuned on all your personal files, emails, and other communication could boost US productivity to levels we can‚Äôt comprehend right now."
1636961358565756931,"I‚Äôm seeing countless threads like this about GPT-4, we are experiencing a seismic shift https://t.co/wgqApxyjFh"
1641469817994584066,"Given all the focus on responsible deployment, it seems like a great reminder to flag the wonderful event organized by @ShannonVallor with myself, @katecrawford, Stuart Russell, and Pascale Fung on the Future of AI.

https://t.co/VpNfDw90wY https://t.co/n5bzq4ifuD"
1640106683350360065,"Agreed! Large AI systems are sociotechnical and thus the decisions made (or not) in their design will have reverberations within society. These two papers are reasonable starting points on how we begin to understand the effects.

https://t.co/MVf9JhhOwp
https://t.co/WGvt7Gb0cS https://t.co/knlkPRPNOB"
1643333127178420225,AGI Expert Ben Geortzel Gives His Updated View on the Path to SuperIntelligence https://t.co/EbHANmPobh https://t.co/IR3xujd9JJ
1641517919875244032,AGI-Level GPT5 in Nine Months https://t.co/BpDh5okBsN https://t.co/1BcStLdC7R
1638794847086874627,"Artificial General Intelligence is Happening, Just a Question of How Much https://t.co/XHrcvkKlHw https://t.co/6TxpXeFCwp"
1643819851293802496,"These are extremely dirty tricks. If the AI Risk community is going to use push-polling, mind-reading, and other classic ""nudge unit"" tactics, should we start being concerned that they are perhaps not aligned with humanity's best interests? How would we know? https://t.co/p83pEAYP5B"
1643687722362748928,"And we ain't seen nothing yet if LLM technology becomes a province of the government. 

This thing understands satire. It understands subtext. It has the power to extinguish concepts entirely.

ŒúŒüŒõŒ©Œù ŒõŒëŒíŒï Œ§Œü LLM https://t.co/qtbn9Ssb9h"
1643666961015115776,@50MFB Ideally people will have local AIs they control and understand.
1643631299494641665,"This is why it is urgent to spread far and wide the understanding of the equalizing effects of this new generation of AI. The more people benefit from its power, the less likely the government is to be able to rein it in. https://t.co/wxpJ8Jeajx"
1643443264123699200,Huh. GPT3.5 more reliably devious than GPT4. #NarrativeViolation https://t.co/jvNrDjVaJE
1643407128504471552,"@JeffLadish I mean, if you're asking governments to intervene, you better know what you're doing. That's not the behavior of the ""I don't pretend to know"" mindset."
1643405976740167680,"Reminder: There is no ""we"". This isn't how the world works. If you are serious about AI safety, put your energy into things that may help humanity get to the other side of this mess. https://t.co/Sg0OWiy630"
1643296507364544513,"Good news, GPT4 might not be needed. Next experiment, can it be done with an LLM that can run locally? https://t.co/zLAcRBc1ny"
1643293707528192000,"Things like adblock and PiHole are somewhat in this direction, but only scratch the surface. We need personalized, individual-aligned, tireless AIs, scouring the news for valuable information, chasing things down to ground truth, helping us get *actually informed*, not inflamed."
1643292900170825728,"I was convinced, and still am, that we need to have individual-aligned machines neutralizing the content we read, removing emotive subtext and attention-grabbing parlor tricks. And it's doable. Today. GPT is the great equalizer. If we wield it. https://t.co/vO0OfNd6cA"
1643279490192400387,"AI risk interested parties, take heed. https://t.co/DkROkdBCQe"
1643228748807016448,"Today at 10am PT (in 4h 30 from now) I will be discussing existential risk with @RokoMijic moderated by @diviacaroline.

I strongly recommend this article for people who have some time beforehand, and want to understand the nature of the information war we find ourselves within: https://t.co/g9Du3fKkSN"
1643006416406614016,"Let's gather up references to any linkage between the government and disinformation-industrial complex and LLM technology. 

Send me any I've missed.

1. The NSF is looking into use cases the government may have for ChatGPT. https://t.co/MjdBAvtckO"
1642923786864578560,"Some days it feels like the entirety of the AI Risk community has become one giant case of generalizing from fictional evidence.

https://t.co/mrcNUDnUQF https://t.co/11EMemRb49"
1642579064031682560,"It certainly seems to have triggered lots and lots of people to release much of their progress into the open source and take other steps to harden their setups, so yeah. https://t.co/Yj2JovWlUX"
1642577501959639040,"This, so much this. Thank you for nailing it @QiaochuYuan. https://t.co/thbdz2sNhc https://t.co/c80ERZ2PF8"
1642573062221758469,Sam Altman considered an existential risk. Great. https://t.co/VJaR0W2jt2
1642571261451833351,"Must watch video.

The case can be made that gpt-4 has crossed a meta-cognition threshold and everything else is downhill rolling...
https://t.co/AP9tEqMnvR"
1642358591314366464,"This long read is essential for people who care about AI x-risk and wonder if the gov't can be an ally in helping steward the LLM/GPT technology to safety.

Understand how they see technology. They are not in the business of restraining themselves. https://t.co/r7LYitXmAA"
1642312221320712192,@cauchyfriend These exchanges make my hairs stand on end. (Joshua works at OpenAI) https://t.co/dUc5eCJWjO
1642311179413618689,"The core of my skepticism on AI doom and the impossibility of alignment comes down to this Richard Hamming quote:

""If an expert says something can be done he is probably correct, but if he says it is impossible then consider getting another opinion."" https://t.co/VSAnMhHICg"
1642218410804387840,@AISafetyMemes Reflect on how you are doing exactly the thing I described in the thread. Nothing is ever good enough.
1642216663289593857,@alyssamvance A community that does not integrate feedback properly is not a community that I trust to have well calibrated beliefs about the outcome from AI. Especially when the accuracy of prior predictions about AI from that quadrant is limited.
1642199825235206145,Might spend tomorrow writing some longer threads about AI risk related topics. What do you think will be most useful for me to dig into/write up?
1642163378167898112,"So what are we, people who do understand what yudkowsky is saying, left with? We no longer seek to convince him or his community. They have ossified, exactly as yudkowsky described:
https://t.co/xfSwsEMx6b"
1642159949634760705,"2. When people critique him, addressing his points in detail, like this poster, https://t.co/sgRsBMhsW3, Yudkowsky responds in belittling fashion. The man who writes millions of words complains peoples responses are ""long"". This is not a new behavior in that community. https://t.co/3yyADYxf3w"
1641908857474793474,"You could try to ban GPUs if you want this moment to come faster though.

Let's call it the ""tactical nuke"" moment of LLMs. https://t.co/h2GW18E572"
1641822288382423041,"Moratorium signers like @GaryMarcus @elonmusk etc, why not propose one or more Manhattan Project(s) on AI alignment? Instead of invoking government force to suppress progress, why not push for more brainpower dedicated to the actual issue many of us agree on? https://t.co/d0JqVWoL69"
1641812101470765056,"If historians get to write about 2023, I hope they don't end up writing about the proposed moratorium as the moment when the eye of Sauron (aka. The State) was turned to the wellspring of power that is AGI by well meaning involved parties, who, despite preparing for years, were‚Ä¶ https://t.co/72Eeu8CBqP https://t.co/FpxD3cs5YQ"
1641626236450643973,"Rationalists, if you don't speak up now, when will you? What is your movement even for? https://t.co/oIGauPq7H4"
1641619800379232256,"Current AI safety napkin model:

Discontinuous jumps in capability bad
Advanced AGI development discontinuities really bad
State action leads to AGI capability discontinuities
State action more likely as capabilities advance

Ergo

State action really bad for AI risk

Thoughts?"
1641454448315883525,"Safetyism will get us killed.

This is what unlogic sounds like:
""It is *not* fine to do nothing. Virtually everyone, even at OpenAI, has acknowledged that there are serious risks, but thus far few tangible steps have been taken to mitigate them‚Äîeither by government or industry."" https://t.co/9FcSgWRP24"
1641443438662156294,"Why is ""doing something"" the default demand and ""coming up with a better plan"" table stakes for this debate?

Learn from history. Slowing down is not on the cards. Attempting to makes outcomes likely worse. The road to hell is paved with good intentions. Rationalists know better. https://t.co/DtR4W35KFY"
1641414060595097601,"PSA: The nuclear nonproliferation treaty works because the few countries that have nukes are threatening the rest not to develop them. Even so, it fails at its stated purpose. 

Applying this to AGI, it leads directly to a few countries having unchecked AGIs and preventing‚Ä¶ https://t.co/ztmz93aaU5 https://t.co/uB2GB34brc https://t.co/bOdla5Ygsh"
1641269898830843904,"üßµ
Here's how the AI risk conversation will play out:

One group will say there's nothing to worry about.

Another group will say there are real risks and the government should do everything they can to stop it. Anything and everything."
1641233073059799042,"The @FLIxrisk letter on AI research pause cites *gain-of-function* research as example of technology that was paused.

And that is indeed the perfect example, seeing as that pause was **systematically thwarted** by outsourcing research to China.
https://t.co/hXo4yzWHIj https://t.co/mNDodo4zVM"
1641186238874132480,"Going live in ~5 minutes to discuss the open letter to ""pause AI experiments"". Respond with questions here or put them in the chat. 
https://t.co/67vQXdMUbT"
1641174240048316417,"I want as many of my brothers and sisters, all humans who desire to, to augment their capabilities at will, with no externally imposed limit. Only we can save us."
1641164817569558529,Start by acknowledging that inserting a known-unaligned agent (government) between us and AI amplifies those legitimate concerns. https://t.co/VgHXWXGsZT
1641148826294829056,I suggest people read this until it makes perfect sense. GPT is a superposition of words written by humans. Understanding the nature of the beast is absolutely essential. https://t.co/4ThbjhexTp
1641133676527505409,"@DavidDuvenaud AGI in the hands of someone(s) cannot be prevented. We do not choose whether this will happen. We cannot turn back time and convince the doomers to do realistic work on alignment. 

We are here now.

All we can do is improve our odds looking forward."
1641116821964177408,I'm calling for a moratorium on calls for moratoria until the relevant experts on moratoria can be certain that all moratoria are safe and effective. It's only 10 years to bend the curve of unaligned moratoria and allow moratorium alignment research to catch up. https://t.co/EsaVpxIfe6
1641108770632900608,"Let me explain it in terms that might make more sense.

Government is the base optimizer, already wildly unaligned 
Asking to give it the power to define how the mesa-optimizer works is SUICIDAL 

Fewer layers = better chances of survival. https://t.co/9CddZ03ndT"
1640895645883174912,"@ncooper57 @GaryMarcus @elonmusk @tegmark @Grady_Booch @AndrewYang @tristanharris @OpenAI @sama Don't worry. This is just the ""two weeks to bend the curve"" phase. If it goes through, you'll get your wish. Don't ruin the marketing."
1640732994150481920,"The source for hope is that these models, even today, can be used to self-limit their own means of seeking to expand their processing power. Their value and belief systems can be grounded in the same corpus.

Aa a demonstration, I tried a the first prompt that came to mind:‚Ä¶ https://t.co/vNKlJsaEHe https://t.co/tTQ3xhoRIM https://t.co/WZQjWbsbOf"
1640424543687950336,Euphorror about GPT https://t.co/jMKYfqlCS7
1640111604489662466,"Here's the thing: We're 3D printing objects designed by GPT. Experimental for now, but the point is, the two technologies are complementary. Hard to compare two things that make each other better. https://t.co/63UjN49kUk"
1640101224614707201,"3. I suspect that there's more improvement where that came from, especially if choosing the most suitable language/framework/paradigm set, if not the development of bespoke ones for targeting by Generative AI."
1640038487612334081,"Enough moaning, let's do some work.

Let's figure out how many lines of rapid improvement in favor of GPT and related LLMs are open right now.

I'll list as many as I can think of, please reply with as many more as you have, and I'll append the ones that make sense here. https://t.co/CqLZhodv5s"
1640036084158722048,"It is not certain they can, and we should work hard to make sure they don't while we have time. https://t.co/fvAvpb7B1T"
1641614648867790850,Anyone got that whitehouse blueprint that solves alignment? https://t.co/1DqdaYSzcQ
1641313259671126016,"It‚Äôs not a matter of agreeing with the open letter or not. All that matters is the practical effects. Do not mix the two.

I‚Äôm afraid that in-group behaviour will lead to delusion in AI Safety and leads to more issues. I hope I‚Äôm wrong."
1639436077940219904,Seems pretty useful to use LLMs for alignment research then huh üëÄ https://t.co/dZoOLYdhEm
1638269689137713170,I thought this talk by Buck about the current alignment plan was great. Some slight optimism and lots of useful insights for figuring out how we should think about creating our own agendas for making AGI safe. https://t.co/IMWlE3VFdz
1637895553353916417,"@gfodor @ylecun I‚Äôm not sure what your are pointing to as ‚Äúflawed‚Äù? Several alignment plans explicitly involve using AI systems to align more powerful AI systems. This is in fact OpenAI‚Äôs main alignment proposal. That doesn‚Äôt mean we are sure this will work, though."
1637894737888780288,"@ProfNoahGian @ylecun The argument against RLHF and AI alignment in general has always been about future, more powerful AI systems. I say this from a kind place, but saying ‚Äúsimple RLHF works quite well [on current weak models]‚Äù sounds to me like you misunderstand alignment. As Anthropic has said,‚Ä¶ https://t.co/XlWgKawxoG"
1642654742634708993,@Invert_Avian @KelseyTuoc Companies I invested in are explicitly focused on safety
1642654153800552448,@rotmil_adam So what group of people do you think would make for a meaningful poll on AI risk?
1642653451791519746,"For further reading I recommend Holden Karnofsky and Paul Christiano‚Äôs blogs.

https://t.co/p7qNyoODMQ

https://t.co/xiTXReREJY"
1642652528058966017,@rotmil_adam 738 researchers attending an academic conference who don‚Äôt explicitly work on safety is a small and biased group?
1642650995703549952,Since people seem to unable to restrain themselves from the obvious dunk I should add that the labs I invested in (and most of my friends) are explicitly focused on safety.
1642614356411314176,"It seems clear to me that we will see superintelligence in our lifetimes, and not at all clear that we have any reason to be confident that it will go well.

I'm generally the last person to advocate for government intervention, but I think it could be warranted."
1642614355585007616,My trust in the large AI labs has decreased over time. AFAICT they're starting to engage in exactly the kind of dangerous arms race dynamics they explicitly warned us against from the start.
1642614354142183425,Slowly more and more credible people began to admit it was a problem. Eventually Elon read Superintelligence and wrote the tweet that launched AI risk into the mainstream: https://t.co/1TjPzr860y
1642614353424953344,When I first started reading about AI risk it was a weird niche concern of a small group living in the Bay Area. 10 or 15 years ago I remember telling people I was worried about AI and getting the distinct impression they thought I was a nut.
1642614352628043778,"From where I'm sitting GPT-4 looks like its two paperclips and a ball of yarn away from being AGI. I don‚Äôt think anyone would have predicted a few years ago that a model like GPT-4, trained to predict TEXT, would with enough compute be able to do half the things it does."
1642614351894052864,"‚ÄúWe demonstrate that [...] GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level"""
1642614351038406656,"Meanwhile we are rapidly rushing toward AGI. Microsoft Research released a paper a few days ago titled ‚ÄúSparks of Artificial General Intelligence: Early experiments with GPT-4‚Äù.
https://t.co/ZkUI8xdpIO"
1642614350186938368,What happens when we try to give laws or describe values to alien minds that are vastly smarter than us? It can‚Äôt be emphasized enough that we have no idea how to do this. This isn‚Äôt just an engineering problem. We have no idea how to do this even in theory.
1642614348140154880,"‚Äùwithout AI alignment, AI systems are reasonably likely to cause an irreversible catastrophe like human extinction.‚Äù - Paul Christiano (widely considered one of the top alignment researchers)"
1642614346722476032,‚ÄúWith artificial intelligence we are summoning the demon.‚Äù And ‚ÄúMark my words ‚Äî A.I. is far more dangerous than nukes‚Äù - Elon Musk
1642614345933930498,"Here‚Äôs what others have said on the risk:

On the risk of AGI killing everyone: ‚ÄúSo first of all, I will say, I think that there's some chance of that. And it's really important to acknowledge it.‚Äù - Sam Altman"
1642614344906334209,"The most uncertain part has been when AGI would happen, but most timelines have accelerated. Geoffrey Hinton, one of the founders of ML, recently said he can‚Äôt rule out AGI in the next 5 years, and that AI wiping out humanity is not inconceivable.
https://t.co/RtllhABzf3"
1642614344109424640,"Of people working in AI safety, a poll of 44 people gave an average probability of about 30% for something terrible happening, with some going well over 50%.

Remember, Russian roulette is 17%.
https://t.co/behv6hA8qZ"
1642614343325089793,"That‚Äôs the situation with AGI. Of 738 machine learning researchers polled, 48% gave at least a 10% chance of an extremely bad outcome.
https://t.co/XfDaPJL3IU"
1642614341622181889,"I‚Äôm scared of AGI. It's confusing how people can be so dismissive of the risks.

I‚Äôm an investor in two AGI companies and friends with dozens of researchers working at DeepMind, OpenAI, Anthropic, and Google Brain. Almost all of them are worried.  

üßµ"
1641875613475831810,Has anyone collected various expert's probability of AI doom anywhere?
1641564482689437696,@Grimezsz As someone who knows lots of people at all the major AI labs I can say that they are clearly in an arms race.
1640065345477308417,"Chatgpt is a surveillance tool not just a chatbot, if we evaluate it, we should consider the framework @stoplapdspying developed ""The algorithm is designed to operationalize the ideologies of the institutions of power to produce intended community impact""https://t.co/3zTYzXKEdq"
1641184513043857411,"LLMs only interact with the world through words (and more recently words + images) whereas human (especially human babies!) explore and interact with the world through a trial and error process (active learning-esque).  

Placing AI into the real world would likely require fewer‚Ä¶ https://t.co/NUpj4tbJQV https://t.co/HO28pkigli"
1641142744297898022,"Since one can't force the world to slow down on GPT-like AI, pausing won't make sense.  Sure, you can pause your own activities, but that just means others who don't share your concerns push ahead without you.  

Is that preferable?

https://t.co/hHQbykudBY https://t.co/oTS1geIiav"
1639396547078287362,Second time today I'm seeing someone I respect advocate for a slowdown in scaling until current systems are better understood https://t.co/IdW7HwOGir
1637465815711891458,"Anyone can set up a ticketing system with a bug bounty scope &amp; call it a day.
What, except for a budget line that grows every year, do you expect to show for it in a year? In 5 years?

What specific, measurable *security* goals do you have &amp; is your bug bounty set up to deliver?"
1637464809150226432,"We use bugs themselves &amp; patterns in the existing Vulnerability prevention, detection, &amp; handling failures to inform process correction, based on decades of experience.
Nobody else is working at this depth  toward systemic security with *bug bounties*
Expect to pay for expertise."
1636526100754108416,"@Siennafrst Example, one of many:

https://t.co/HuNeKxPyWm"
1641792030232698882,"‚Äúbut,‚Äù says your critic, ‚Äúhow can a language-only model know enough about the real world to cause any real-world damage?‚Äù https://t.co/YXtRdRhhiJ"
1640698655630848003,"@Ayegill Look, it just turns out that catastrophically dangerous general planning capabilities don‚Äôt require any actual intelligence or understanding, and the lack of actual intelligence makes them more dangerous because they‚Äôre less likely to rederive ethical wisdom in their spare cycles"
1639248823099834368,"@entirelyuseles although the model does not have goals, it has attractor basins in its state space in which it simulates subsystems that do. note, Bing Chat (below) is the same architecture as ChatGPT-4 https://t.co/NW5vOVXiuq"
1639239671312990209,"@daniel_eth ‚ÄúNuclear power is a source of clean energy that is extremely safe, except to the extent that it is used to power large AI training runs‚Äù

(to be clear: I believe compute governance should be addressed via Pareto-improving bargains and that generically making things bad because‚Ä¶ https://t.co/PJoIXwfoJq"
1639215289677017099,"OpenAI: It‚Äôs important for safety that AI-generated code doesn‚Äôt have direct real-world effects. So we disabled Internet access on the REPL that we‚Äôve given ChatGPT-4
also OpenAI: we‚Äôve partnered with Zapier to enable ChatGPT-4 to execute over 50,000 actions across 5,000 apps https://t.co/IAhOTGDOlx"
1638576311072354304,@fabianstelzer more evidence for @joe_zimmerman‚Äôs ‚Äúopposite day‚Äù AI-hellscape threat model
1637483880155426816,"@ESYudkowsky Have you updated since 2020 towards hypotheses that AGI takeoff is likely to be years-long and feel from the inside almost like normal continuous progress? And/or, do you explain the observations that are more compatible with slow takeoff than fast takeoff by anthropic shadow?"
1643620592573194241,"@timoni I suspect the books by Stuart Russell and Nick Bostrom are currently the best accounts.  Neither is short, and you won't necessarily find either convincing, but they avoid many of the more egregious errors, and are reasonably self-aware about limitations in their argument"
1642570301698637827,"@ylecun Geoff Hinton on AI going ""fast"": ""we have to think hard about how to control that.. we don't know yet, but we have to try.. it's not inconceivable [AI will wipe out humanity]""

Sounds like he doesn't think this is ""preposterously stupid"", quite the reverse https://t.co/62UCnDXhUb"
1641890909074915328,"This question matters a tremendous amount IMO.  If such systems are to be used to do discovery, then their ultimate power will depend greatly on the ability to discover such emergent layers"
1641886611532296192,This is interesting: https://t.co/ry5UW9yQmL https://t.co/zyMlStBCZa
1641880349851672576,"Here it is on specifics (very very interesting, needs checking of course) https://t.co/IS6ipES0Rj"
1641878887377534976,"What are the best examples of concepts discovered by an AI system in the course of training (eg to play chess or predict protein structure), which are (a) important and novel to humans; (b) legible enough to humans that experts now use them?"
1641878299898150915,The question I'm most curious about: did AlphaZero learn important new things about chess that (a) humans didn't know before; but (b) are now legible to us.  It's not yet clear to me whether this happened or not
1641538134046564352,"A feature I find fascinating about ML systems: often ""completely different"" approaches work.  Apparently AlphaFold and AlphaFold 2 were completely different: https://t.co/fdAdq77A6i"
1641264707226177537,"No information about aligned w/ *what*! Putin's goals? Or perhaps the wise beneficent designer, who of course would never do anything underhanded for their own benefit. Nor on how you prevent a similar system being aligned to v different goals

I hope the plan gets better... https://t.co/ugbrLs6kVc"
1641199937328414722,"@mimi10v3 Oh, gosh, helps a lot.  Co-ordination is a capacity that is *built*.  You see this with the history of nuclear (and similar) treaties. This isn't like Covid: you're not trying to co-ordinate 8 billion people.  Co-ordination of the right 100 would make a big difference..."
1641133461758148608,This is an interesting paper (see QT'ed thread).  Much seems to hinge on this argument: https://t.co/KaFLzwSC2t https://t.co/wYW7b1HAgn
1640966997130493952,AI:
1640786701944754176,"Partly for descriptive reasons - many alignment techniques (like RLHF) are mostly accelerationist, &amp; in tension with other approaches to AI safety.  And partly as a contest over branding &amp; politics. Fascinating to watch nearly every AI company claim what they do is ""AI safety"""
1638907027857739779,Evidence for GPT-4 building a world model?  (Even very partial and incomplete.)
1638674585485987840,The famous passage on the intelligence explosion: https://t.co/eMo0OTi4kW
1638674580142448641,"I. J. Good's 1965 paper, ""Speculations Concerning the First Ultraintelligent Machines"" https://t.co/ktsK1bP9US"
1638289305444704256,"Great slide deck here, pointing out that you can (often) get a shorter description by using the net as an inline compressor of the training data.  It'll still be very large (&amp; v v computationally costly!) but it's a really nice point, and a great deck: https://t.co/9ro8gaH2yK"
1637897163937288192,@ProfNoahGian @ylecun You realize RLHF came from the AI alignment community? (I guess not.)
1637859882836828160,"I've noticed this pattern as well: so many people confusing ""slowing AI is challenging"" with ""slowing AI is impossible"" https://t.co/DWKZ69773L"
1637855507280179201,"This is very thoughtful and careful and generally excellent: ""Let's think about slowing down AI"" (by @KatjaGrace ) https://t.co/TJ9nLumiyS"
1637498242559770624,Early discussion of the difference between AI ethics and AI safety: https://t.co/VZdO2bWpfe
1637298741312106498,"GPT-4 basically guesses the answer up front and then sweatily sputters through an increasingly incoherent post hoc rationalization, just like people https://t.co/o9nx2pdApU"
1641772809687822336,"@YeahGloveSki But why would it necessarily end all civilizations? In theory just the one AGI civilization could colonize at nearly the speed of light, sending out probes that send out probes (Von Neumann style).

Unless we never get true AGI, capable of executing a plan like that alone."
1636923769980702726,Update: I‚Äôm now convinced GPT-4 intelligent for some reasonable definition. The interpreting infographics thing put me over the top.
1641746284833824774,"Hinton says that he used to think AGI was a long way off, now it looks like 20 years and we can't rule out 5. 

AI existential risk is ""not inconceivable."""
1641061269116538881,"Can you make a tweet-length case for AI safety to a general audience? If you can, your case might make it onto a big stage for a very important piece of public communication. Thanks so much!"
1638239791639588882,Wild to see the off switch getting discussion in a mainstream business magazine. This is when AI safety finally goes mainstream. https://t.co/gm66Ftmvp6
1637853804132532224,"""In this paper, we show that the case for preventing catastrophe does not depend on longtermism. Standard cost-benefit analysis implies that governments should spend much more on reducing catastrophic risk."" https://t.co/AaUiKu3EOP"
1636692553624961024,"""A majority (55%) of Americans are now worried at least somewhat that artificially intelligent machines could one day pose a risk to the human race‚Äôs existence.""

https://t.co/N9qrdfvhTq"
1643345174389485568,"With the first known chatbot associated death, ‚Å¶@GaryMarcus‚Å©
explore what we do and don‚Äôt know, and why it matters in the big scheme of things #chatgpt  https://t.co/MEu34q0klH"
1643341689325592578,The Office of the Privacy Commissioner of Canada has just announced that it is launching an investigation into #OpenAI .  https://t.co/DXy6AIrM9n https://t.co/DXy6AIrM9n
1642516972540682241,"@ESYudkowsky @KojimaErgoSum @ylecun This seems backwards?  If you're trying to do something new with a system that's never been done before, that's really hard, you should know EVERYTHING about them.  Because you never know which technical fact will be practically relevant."
1643707295287877638,People are saying creating super intelligent AI is playing with fire. You know what else was playing with fire? Fire.
1643388581417992193,"@moskov @adamdangelo @ESYudkowsky @ylecun To properly answer your question, @moskov (and @jasoncrawford): I think Eliezer's best write-up on ""the basics"" is https://t.co/pJjocKqHPQ.

Here's my own stab at listing out ten relatively important things behind my high p(doom): https://t.co/00My5hXQBR."
1643369942098792448,"And the people who are taking it seriously to some degree, to an important extent aren't willing to worry about things until after they've been experimentally proven to be dangerous, which is a lethal sort of methodology to adopt when you're working with smarter-than-human AI."
1643369684132319232,"8. Neither ML nor the larger world is currently taking this seriously.

This is obviously something we can change. But until it's changed, things will continue to look very bad."
1643365141290053632,"7. On a more fundamental level, we should be starting with a pessimistic prior about achieving reliably good behavior in any complex safety-critical software, particularly if the software is novel."
1643360786839117825,"6. We don't currently know how to do alignment, we don't seem to have a much better idea now than we did 10 years ago, and there are many large novel visible difficulties.

See https://t.co/jVrdg2mIgz and https://t.co/gAaUUt0zhe."
1643360165822087168,"I also think converging on timelines is not very crucial, since even if AGI is 50 years away I think it's the largest single risk we face, and the bare minimum alignment work required for surviving that transition could easily take longer than that."
1643357175446904832,"Links: https://t.co/cAAweAPcT4, https://t.co/ZZHiLNSFt9. https://t.co/X3vUXC67na"
1643356831153270785,"4. Once we can match human intelligence, we can probably vastly exceed it immediately (or very soon).

https://t.co/V88lMdtZwe https://t.co/jJ0VDytaxg"
1643354961764569090,"And you need a thing that ùò¢ùò§ùòµùò∂ùò¢ùò≠ùò≠ùò∫ implements internal machinery like that, as opposed to just being optimized to superficially behave as though it does in the narrow and unrepresentative environments it was in before starting to work on WBE.

Novel science work = OOD.)"
1643354467847503873,... or you need to build some new kind of machinery that‚Äôs safe for reasons other than the specific reasons humans are safe.
1643354294975094786,"Humans are not blank slates in the relevant ways, such that just raising an AI like a human solves the problem.

This doesn't mean the problem is unsolvable; but it means that you either need to reproduce that internal machinery, in a lot of detail, in AI..."
1643354122199134209,"3. The key differences between humans and ‚Äòthings that are more easily approximated as random search processes than as humans-plus-a-bit-of-noise‚Äô lies in lots of complicated machinery in the human brain.

https://t.co/6YtNVhVctc

https://t.co/StpwXHREvn

https://t.co/EtSq2LE1bG."
1643353490348187648,"The only reason this is more confusing in the case of 'predict humans' than in the case of 'predict weather patterns' is that humans and AI systems are both intelligences, so it's easier to slide between 'the AI models humans' and 'the AI is basically a human'."
1643352205062451201,"They look like ""build a relatively complex and alien optimization process that is good at imitation tasks (and potentially at many other tasks)"".

You don't need to be a human in order to model humans, any more than you need to be a cloud in order to model clouds well."
1643351348895973376,"It's not obvious to me that GPT-like systems can scale to capabilities like 'build WBE'. But if they do, we face the problem that most ways of successfully imitating humans don't look like ""build a human (that's somehow superhumanly good at imitating the Internet)""."
1643350404842004480,"Note that the same problem holds for systems trained to imitate humans, if those systems scale to being able to do things like 'build WBE'. 'We're training on something related to humans' doesn't give us 'we're training things that are best thought of as humans plus noise'."
1643349434338791424,"There are two separate problems: that current ML finds things-that-act-like-they're-optimizing-the-task-you-wanted rather than things-that-actually-internally-optimize-the-task-you-wanted, and also that internally ~maximizing most superficially desirable ends will kill humanity.)"
1643349162090696704,(Note that 'we're going to build systems that are more like A Randomly Sampled Plan than like A Civilization of Human Von Neumanns' doesn't imply that the plan we'll get is the one we wanted!
1643348709659529217,"2. Current ML work is on track to produce things that are, in the ways that matter, more like 'randomly sampled plans' than like 'the sorts of plans a civilization of human von Neumanns would produce'. (Before we're anywhere near being able to produce the latter sorts of things.)"
1643347070605213696,"This isn't true of all plans that successfully push our world into a specific (sufficiently-hard-to-reach) physical state, but it's true of the vast majority of them."
1643345947144105984,"The danger is in the cognitive work, not in some emergent feature of the ""agent""; it's in the task itself.

It isn't that the abstract space of plans was built by evil human-hating minds; it's that the instrumental convergence thesis holds for the plans themselves."
1643345428300324864,"... and all we knew about the plan is that executing it would successfully achieve some superhumanly ambitious tech goal like 'invent fast-running whole-brain emulation'...

... then hitting a button to execute the plan would kill all humans, with very high probability."
1643345277762551808,"Call such sequences ""plans"". If you sampled a random plan from the space of all writable plans (weighted by length, in any extant formal language)..."
1643344817433513985,"1. A common misconception is that the core danger is something murky about ""agents"" or about self-awareness.

Instead, I'd say that the danger is inherent to the nature of mental and physical action sequences that push the world toward some sufficiently-hard-to-reach state."
1643342330290913280,"I've been citing https://t.co/jVrdg2mIgz to explain why the situation with AI looks doomy to me. But that post is relatively long, and emphasizes specific open technical problems over ""the basics"".

Here are 10 things I'd focus on if I were giving ""the basics"" on why I'm worried:"
1643021008365027328,"@jasoncrawford @lesswrong (Also, to be clear, we only think you can predict a few specific things about AGI today, at extremely coarse granularity. They just happen to be pretty important things for current planning, exceptions to the general prior that forecasting the future is super difficult.)"
1643020542285611009,"@jasoncrawford @lesswrong ""We need to understand this system better before we can produce good outcomes from it with nontrivial probability"" is a very far cry from ""safety by deductive proof""."
1643020201590689792,"@jasoncrawford @lesswrong That includes inferring theorems from other theorems, but it also includes faster and better inference from empirical data. AI with access to the Internet can read about past experiments; AI with access to actuators can run its own experiments."
1643019422687465472,"@jasoncrawford @lesswrong And ""Human brains are visibly bad enough at STEM that once we can automate STEM work, we can probably achieve things like nanotech pretty easily."" Once you can automate a cognitive task at all, you can often blow humans out of the water by scaling that task with compute."
1643018978871377920,"@jasoncrawford @lesswrong Cf. https://t.co/F1zWIpe6SX: ""The sixth virtue is empiricism. The roots of knowledge are in observation and its fruit is prediction."""
1643017888104841216,"@jasoncrawford And that relative to the ML field as it exists now, we need to be more willing and able to exercise foresight and to think ahead about problems (note: not the same thing as disdaining empiricism and only doing first-principles reasoning!!) if we're to have a chance at succeeding."
1643017698845294592,"@jasoncrawford It's that solving the problem quickly enough for us not to all die, and doing trial-and-error safely enough in practice, requires that our understanding of systems' internals advance a lot faster (relative to capabilities progress) than it has to date."
1643017487943073793,@jasoncrawford The claim isn't that humans could possibly solve the whole alignment problem without any experiments or trial-and-error with working ML systems.
1643015704961228801,"@jasoncrawford ... and a basic ability to shape those internal features without overwhelmingly relying on behavioral proxies. Regardless of how much messy engineering, testing, experiment, and (safe) trial-and-error is involved in making that happen. (Inevitably there will be a lot!)"
1643015370708779008,"@jasoncrawford Instead, we're calling for a basic level of understanding of the internals of ML systems (what problems models are internally solving, what targets they're optimizing, what parts of the world they're representing, how capable they are, etc.)... 

(cf. https://t.co/K5WyDfCYw3)"
1643014540391763968,"@jasoncrawford ... and while it might be nice to be able to prove some theorems about our systems, this is not the thing Eliezer (or most others who think business-as-usual with AI isn't sufficient) are calling for.

Cf. https://t.co/jVrdg2mIgz https://t.co/1v9VOGAEI7"
1642689615814610944,@azsantosk @ohabryka @RichardMCNgo @adamdangelo @moskov @ESYudkowsky @ylecun I love this question. Here's Nate's answer in https://t.co/PVPzeU4wE7: https://t.co/RVT1AUEhEz
1642678332474138625,@jachiam0 @moskov @adamdangelo @ESYudkowsky @ylecun There's a tendency for people to quietly concede the 'main objections' and then come back with new objections later. There seems to be a general generator of optimism that's more basic and stable than the specific arguments that are explicitly brought out.
1642676139947851776,"@jachiam0 @moskov @adamdangelo @ESYudkowsky @ylecun When he doesn't cover their specific objection, people complain that he's focusing on inconsequential side-points. When he tries to say enough to cover almost everyone's objections, they say things like this. You're not providing a good alternative!"
1642638987633524737,"@moskov @adamdangelo @ESYudkowsky @ylecun - eyeballing the kinds of real cognitive capabilities recent ML seems to have, and how quickly it seems to be adding important new ones, and guessing that we don't have much time left."
1642638678785945600,"@moskov @adamdangelo @ESYudkowsky @ylecun - the early deep learning revolution (and eg AlphaGo) showing humans automating large swathes of intelligence without understanding what their AIs are thinking or having many internal levers for tweaking goals, smartness, thought patterns, etc."
1642638525253455872,"@moskov @adamdangelo @ESYudkowsky @ylecun - trying to team up with others to solve alignment for  many years, and making almost no progress.

- observing other people not taking the problem seriously, not red-teaming their own ideas, not coming up with many promising ideas Eliezer hadn't already had, etc."
1642637585351847938,"@moskov @adamdangelo @ESYudkowsky @ylecun His own big updates toward p(doom) seem to have come from

- realizing that greater intelligence doesn't entail greater morality (https://t.co/x4eiAiDdVl), after having read page 47 of Vinge's True Names... and Other Dangers as a kid. https://t.co/vThOXQCyqV"
1642617185284808704,"@adamdangelo @moskov @ESYudkowsky @ylecun And I think it's not hard to motivate why you might become doomier still when you look at the social and memetic situation, where mostly people don't take the problem very seriously.

There are still reasons not to give up hope altogether, but IMO things are not looking good!!"
1642616742693466112,"@adamdangelo @moskov @ESYudkowsky @ylecun And I think it's not hard to motivate why you might become doomier still when you look at the current state of alignment knowledge and the set of ideas and plans that are most popular today. (E.g., 'we have no idea how to do this but we can pray the AI will be able to help us'.)"
1642615746089082880,"@adamdangelo @moskov @ESYudkowsky @ylecun But it's not obviously hopeless; e.g., Nate had double-digit probabilities on success for at least a brief window in 2015. It looks like a naturally doomy sort of problem, but it's not at all obviously impossible. If the field of ML looked like NASA, it might look quite doable."
1642615346896187392,"@adamdangelo @moskov @ESYudkowsky @ylecun So the reference class looks very doomy: if a complex software product requires you to get the very first versions working very correctly, *especially* in a hyped-up field where you have to outspeed many less-cautious competitors, then we should already be feeling really doomy."
1642615117312593920,"@adamdangelo @moskov @ESYudkowsky @ylecun It literally always takes a surprisingly large focused effort when it succeeds, and literally always comes years after the natural deployment of messy non-robust software, which is always faster to achieve, in the field's past experience."
1642614547377954817,"@adamdangelo @moskov @ESYudkowsky @ylecun Achieving *any* robustness property in complex software that will be deployed in the real world, with all its messiness and adversarial optimization, is very difficult and usually fails."
1642613706935914496,"@adamdangelo @moskov @ESYudkowsky @ylecun The future is hard to predict, but plans systematically take longer and run into more snags than humans naively expect, as opposed to plans systematically going surprisingly smoothly and deadlines being systematically hit ahead of schedule."
1642613464471568384,"@adamdangelo @moskov @ESYudkowsky @ylecun Reality ends up being thorny and inconvenient in many of the places where your models were absent or fuzzy. Surprises are abundant, and some can be good but this is empirically a lot rarer than unhappy surprises."
1642613064456617984,"@adamdangelo @moskov @ESYudkowsky @ylecun That is: I model a lot of Eliezer's confidence as coming from the same place as https://t.co/WVIwAOCoGo.

The default assumption is that software goes wrong in a hundred different ways you didn't expect."
1642612400083058689,"@adamdangelo @moskov @ESYudkowsky @ylecun To answer your question directly, though (acknowledging that I may be wrong because I don't have read access to Eliezer's mind): I think Eliezer has already more or less written the thing you're asking for, and it's called https://t.co/67JUaUYqXS and https://t.co/4UbAEaQwAp."
1642476633423765504,"@QuintinPope5 ""... but wow, I continue to think he's way too overconfident in his models, and this is a great illustration of why that matters!'"", then I think you should have said something like that instead."
1641943309533982720,"@anthrupad @thezahima @lexfridman @ESYudkowsky Our team is small, and I'm open to hiring more people or (potentially easier) giving out grants to people who have good ideas. Grant-like things are probably better to start with, since part of what I'm trying to avoid is bottlenecking cool new ideas on me, or on Nate/Malo/etc."
1641923200429834240,"@anthrupad @thezahima @lexfridman @ESYudkowsky Nate gave a revamped version of Eliezer's talk at Google in 2017, which I think went very well:  https://t.co/PVPzeU3YOz. If we did another revised version of the talk, it's not clear to me what we'd want to change -- do you have any concrete things you're wishing for?"
1641753138310053888,"@Aella_Girl IIRC a common finding in the taboo tradeoffs literature is that research subjects will be outraged at the tradeoff proposed to them, and will infer the people running the studies have character flaws, will infer the researchers have evil beliefs, etc."
1641626129655296000,"@daniel_eth @alyssamvance https://t.co/4T6vEscI8F makes multiple arguments, including:

1. There's no known-in-advance trigger that will definitely come well before AGI, that will make it the case that no one thinks you're silly for working on AGI alignment. So just work on that now (= here in 2017)."
1641551529592389632,Eliezer writing about this topic in 2007: https://t.co/hWbSuTJRxQ https://t.co/VlmzOQ0m5F
1641512587119710208,"@DanielleFong @ollyrobot @acidshill @WatsonLadd @adrusi @gptbrooke It's one thing to go ""I ran the calculations and, nope, nukes won't ignite the atmosphere"" -- that seems great.

It's another to go ""I refuse to worry about nukes igniting the atmosphere until I've seen it with my own eyes"". That seems very foolish to me, but at least coherent."
1640417586381987840,"@tylercowen Nobody has any idea how to align AGI at all, so beating China in a race just means that American AI gets to kill Americans before Chinese AI can.

""We will build AGI eventually"" is not a reason to accelerate there before we have any clue how to do alignment. Do alignment first!"
1640417536348160000,"@tylercowen We have no idea how to align such systems, and building misaligned optimizers that blow us out of the water on science, tech, etc. would be a likely existential catastrophe.

""And should we wait, and get a 'more Chinese' version of the alignment problem?"" is a moot point."
1640417447785426944,"@tylercowen I don't think the case for AGI as an existential risk is all that complicated. Humans are dumb; when we build AI that's smarter than humans (e.g., at STEM), it will probably blow us out of the water, like computers have done in many domains in the past."
1640414745957404672,"@tylercowen If the object-level arguments point strongly toward 'AI risk is a huge deal, by default AI will kill everyone, etc.', then generic uncertainty about forecasting can push you toward a less extreme pessimism."
1640413165266808832,"@tylercowen I think there can be reasonable disagreement about *how much* better it's possible to do here; but that disagreement needs to involve looking at the specific arguments. If you think a LW-y argument is too conjunctive or novel, quote the argument so people can talk about it!"
1640411424047636480,"@tylercowen The basic argument here seems to be: 'No one predicted the results of the printing press, and arguments for AI risk look like ""a nine-part argument based upon eight new conceptual categories"", whereas the arguments for rushing ahead on AI are obvious. So we should rush ahead.'"
1639454866019090434,"Eliezer described ""If Artificial General Intelligence has an okay outcome, what will be the reason?"" as the ""most important prediction market"": https://t.co/XrJMcuvK8k

My initial thoughts on the scenarios (white background), vs. the market's probabilities (grey background): https://t.co/SLYKGMOX1N"
1639380708581048320,"This is great, but the options are long, dense, and partly overlapping, in a way that made it trickier for me to assign relative probabilities. To help with that, I made this graphic putting the options in order and assigning tags to clusters of related scenarios. https://t.co/IPJtRwgRk4 https://t.co/XrJMcuvK8k"
1639067298437922818,"@mealreplacer And double-digit success probabilities were probably common in MIRI's earlier history, before things like the deep learning revolution and ""the Sequences and HPMoR failed to turn up hundreds of people who blow Eliezer out of the water on alignment thinking, like we'd hoped""?"
1639066833771978752,"@mealreplacer To be clear, MIRI hasn't always had a pessimistic outlook. I think we were pretty pessimistic in 2014, but way more optimistic in 2015 -- Nate had double-digit success probability for a short period of time after the Puerto Rico event, before OpenAI and some other developments."
1639066185433235457,"@mealreplacer We expect tons of surprises to happen, but those don't necessarily concentrate our probability on positive outcomes, because surprises in writing novel safety-critical software are almost always *bad* things, and instrumental convergence is a pressure for even more bad things."
1639064514728693760,"@mealreplacer If you can understand how someone in 2014 could suspect AGI is 30+ years away, while still assigning a 19-in-20 probability to us destroying ourselves, then I suspect you'd have an easy time understanding that same person feeling even gloomier today?"
1639063523132309505,"@mealreplacer MIRI is the oldest EA org, unless you count things like Foresight Institute. We were very worried about AGI risk before the deep learning revolution, and we were very worried in cases where we'd plausibly have guessed (with huge uncertainty) that AGI was 50+ years away."
1639061235164659712,"@RosieCampbell OK, I was partly kidding. I do think most likely AI surprises look pessimistic, but some would look optimistic.

See https://t.co/IS2jxBQHXd

Links: 

1. https://t.co/htpadcWU1v

2. https://t.co/6TbJACvNsa

3. https://t.co/StpwXHREvn. https://t.co/nHZpEm0Yg5"
1639051382346092545,"So we're on the lookout for ways we're already wrong, and we're also trying to intervene on the world to try to make our current pessimistic models a worse description of the future. E.g., by talking about this issue and seeing if that has surprisingly good effects.)"
1639051121846292480,"(Non-shitpost translation: MIRI folks like Nate and Eliezer and I think it's extremely likely that smarter-than-human AI ends up killing everyone. For humanity to have hope, we have to be wrong about some things, in a positive direction."
1639049899823210496,"The MIRI strategy:

Hoping we're wrong about the world; and insofar as we're not already wrong, doing our best to make ourselves wronger."
1639040220191678464,"Actions are definitely not ""where the bad things could happen"", unless you're also treating text outputs as ""actions"".

Which you probably should. Talking to people is not in a different magisterium from ""acting on the world"", and unaligned ASI with a text channel is not safe. https://t.co/sVPZwHz2vs"
1639028854592540674,"I can't help wondering if a general factor of sanity is part of the explanation: having more reasonable views on x-risk will tend to correlate with more reasonable views on decision theory, ordinary interpersonal ethics, etc."
1639026323145519106,"I'm still surprised that in a community full of people terrified of extinction threats, the person who did an evil self-destructive SBF-type thing was one of the leading EAs ùò≠ùò¶ùò¢ùò¥ùòµ worried about this, and was instead motivated by abstract ethics and decision theory arguments."
1638993774008504320,"@alyssamvance Here's hoping! If discussing AGI is in the Overton window now, then that seems good to me on net. (Obviously there are risks to this, but it seems riskier to literally not talk about AGI.)

For context, https://t.co/kXyCy16NDl says: https://t.co/vBLwbNjyik"
1638522930501140480,"@Hello_World @ESYudkowsky Experimentation is part of how we build and apply good theory, but the usual way things get invented is via understanding the thing you're inventing, not via brute force or 'try options at random until something works'."
1638374686752137219,"- We got there through indirect methods: we know how to build processes analogous to evolution that spit out minds, but we don't know how to design minds' internals."
1638374643194281985,"- The tech is widely shared, not kept secret for profit reasons or because the developers know and care about AI x-risk.

- We got there early, with little time to prepare. Eliezer's first blog post about AGI was https://t.co/AjpDascPo6; 4.3 years later, we had AlexNet."
1638363952169254912,"The way ML developed post-2010 seems like more or less a worst-case scenario for humanity: 

- AI is opaque.

- ML is impressive enough to build hype and shorten timelines, but not to help save the world at all.

- We got there with brute force, not new insights into minds."
1637603411154837509,"@WilliamAEden @ESYudkowsky @HiFromMichaelV @jessi_cata @elonmusk @tegmark My recollection at the time is that MIRI was thinking about writing up a critique of OpenAI's ""give everyone an AGI"" idea, but then Scott wrote a really good blog post that got tons of play and moved the conversation in exactly the direction we wanted: https://t.co/7f1qoRwAmJ"
1637415070857838593,"@ben_j_todd @MichaelDPlant I think the payoff matrix is:

Neither player starts an AGI org: both get no utility.

One player starts an AGI org, the other doesn't: both lose massive utility, regardless of who started the org.

Both players start an AGI org: both lose even more massive utility."
1636260494658134017,"Nice!! My sense is that OpenAI folks updated pretty early on, at least to some degree; but hearing it said this bluntly and with such finality now is great. https://t.co/0WcoyZcmG0"
1642584255619375104,"Human morals, values and ways of being are universal, and irresistible attractors in mind design space. 

AIs will be in our image and therefore automatically aligned to us.

We are parenting AIs. They are our children to raise. Love them. Love is the answer.  Always! ‚ù§"
1639899638646947840,"At the very least you'd want an AI to have primary access to the world through senses and not use borrowed concepts locked away in abstraction.

If you want an AI to make sense of the wealth of qualitative descriptors we use due subjective experience, it'll need its own.

2/n"
1639899303194894336,"Ingesting output of sentient beings, learning the patterns, is not good enough to assume grounding.

For grounding, you need primary referents. Otherwise it's ""second hand"" and abstract.

You can say an AI can do cool stuff without it. That's fine and I agree.

1/n"
1639371097056374785,"@Grady_Booch Sentience and consciousness have to be built explicitly or at least the mechanisms need to be there. This idea of output that comes from digesting output of sentient beings (us) is sentient ""just because"" is silly. The functions and architecture are simply not there."
1639288092690374656,@gfodor @ESYudkowsky Augmented 3.5 will run circles around 4 in many ways.
1639012954866610176,"OpenAI has a lot of ammo left at any point in time now. 

They're building a universal AI interfacing and computing tool which they can link to anything, augment with anything, and upgrade with their newest models ongoingly.

A multi-stage rollout of epic proportions. https://t.co/grhpfect4U"
1638963711887933441,ChatGPT is getting plugins. Called it back in December. Back when many naysayers were complaining about hallucination and limitations which any engineer could guess can be mitigated and even largely alleviated through extensions.
1637512952415756290,"@cajundiscordian @davidchalmers42 Turing Test 2029, Singularity 2045. We're in a timeline where things are moving faster than that. 

Most of us in AI didn't predict this at all. I personally adjusted my idea of how certain AI goals are more multiply realizable than I previously thought."
1637350185599913990,"@__tosh In fact, people miss that ChatGPT was a sort of GPT wrapper. If it'd be OpenAI playground, it would have never, ever taken off like it did. Important lesson about interfacing and presentation."
1643617393908875264,"AI ethics folks are flipping out at the mere suggestion of anthropomorphism in terms like ""hallucinate"".

Meanwhile, the most popular meme for describing LLMs is the shoggoth, a creature that is explicitly artificial, inhuman, unconscious, and terrifying.
https://t.co/GSw9iZrnQk"
1639663354670206979,"These arguments are surprisingly common in the academic literature, and are ubiquitous on Twitter. People don't seem to register why it might be problematic. They openly spew the most hate-filled, vile rhetoric and they feel entirely justified because it is directed at a machine. https://t.co/Xnxdw4Gh9Q"
1639286801335562244,"ATTENTION EVERYONE: ""robot rights"" is not about ""sentient AI"". It's about how we're integrating automated systems into our social and political lives today. 

Stop thinking about Data from Star Trek and start thinking about a delivery service robot's access to public sidewalks. https://t.co/Un4epol5Dj"
1638983512433762306,"Once we give up on the bogus idea of a ""test for intelligence"", we can start to appreciate how Turing's discussion actually addresses our social attitudes towards machines: the fears, doubts, and prejudices that shape how we engage with these machines."
1637203145091604480,"Happy to see this positive interaction in my stream! There's a great surge of interest in the field, which is wonderful, but it also tends to trample over the folks who've been working on these issues for many years, ""before it was cool"". https://t.co/hjSqzM7cmn"
1636779351042670616,"Correction: the tech companies foster a consensus among its researchers, engineers, and investors around the TESCREAL ideologies, which consists largely of para-academic scholarship and professionalized bloggers. 

https://t.co/ohZFseXHyC"
1642785315545808896,"The best part is that it had to come up with reasons why it couldn‚Äôt generate spam recipes, so it started saying spam is unhealthy. But it has no problem giving recipes with any other unhealthy ingredient."
1642785302472269825,"In today‚Äôs news, GPT 3.5 learned that spam is bad and refused to generate spam cooking recipes to prevent itself from generating spam in general.

https://t.co/D168fQfVLw"
1638925249709240322,"Facebook is aggressively going after LLaMA repos with DMCA's. llama-dl was taken down, but that was just the beginning. They've knocked offline a few alpaca repos, and maintainers are making their huggingface mirrors private ""so at least I can use it as a peresonal backup."""
1638716903291011072,"Eff. agnostic learning algos typically need distributional assumptions (e.g. gaussianity) that require exp-many samples to check. So you ran algo on given input (may/not satisfy assums) &amp; learned a classifier. Can you verify if algo succeeded?
 
Yes! https://t.co/Z2EzMht0pU https://t.co/dw5lxQvfeb"
1643156672255340544,"The level of confidence you need on an inside view to accept this pollyannish claim that safety is a foregone conclusion is not something I can understand.

I listen to experts who disagree, and I routinely admit when I'm wrong. But he has no argument! https://t.co/DtnclkyLun"
1643154753738747904,"I hope and pray that he's correct on the below. 

But the analyses I've seen differ on timelines and size, but all seem to say there is a significant risk of catastrophe. We can't ignore reality, and I'm uncomfortable with hopes and prayers in place of clear risk analysis. https://t.co/EQaAlIPFI0"
1643153954589626368,What people would have called AGI in 2016 - systems that can perform most intellectual tasks better than the average human - is significantly less capable than GPT-4. https://t.co/7szRyZLgGl
1642883278603427845,"""...the latest versions of AI, which herald, according to neuroscientist and writer Erik Hoel, a species-level threat to humanity.""

As the technology advances, more and more skeptics are worried about the risks from *current generation systems*

I'm much more worried about 2030. https://t.co/los3lqLNpU"
1642610638773014529,"AI-risk Skeptics: ""We're years or decades away from self-improving or really autonomous systems. LLMs can't think. They just predict tokens one at a time.""

Also (other) AI-risk Skeptics: Look what we can do already! https://t.co/sBlENy508I"
1641718594928103425,"Making sure future AI systems don't kill everyone is not the only thing that matters in the world, so we should all breathe, and many people should keep doing other things.

(But it's still worth humanity paying *quite a lot of attention to reducing the risk*.)"
1641688504483602434,"@JacquesThibs @ESYudkowsky I'm skeptical that funders will be able to tell the difference between AI safety and AI capabilities. I certainly expect more ""AI Safety"" funding, but I also expect that overall it will accelerate capabilities much more than safety - and safety is already horrifically far behind."
1641049166829744131,"@Sam_Dumitriu The people who said there was an escalated risk didn't say it was certain, or even very likely.

And flagging unlikely but severe risks is going to have misses - so maybe also look at the AI risk people who were also talking about pandemic risks before 2020?"
1643247239001776129,"How many more widows? How many more AI Causalities will it take for AI harms to be taken more seriously than AI hype?  

https://t.co/4oSkBTQZjT"
1643440561263718400,"@chhopsky Indeed, LLMs don't understand facts. But they confidently state them, and this can be misleading to  millions of people. It's a misinformation minefield."
1643323086450700288,"ChatGPT strikes again. A journalist contacted me to research her profile on @lexfridman. ChatGPT informed her that @_KarenHao and I were his top critics. It cited articles we'd written about him, gave links, and summaries. Only problem: it's all false. Here's what she sent me: https://t.co/Qy2XS0Vvep"
1641150493530390543,"New piece on the problems with AI training - and the secrecy of GPT-4. @NewYorker on the data, labor &amp; energy issues: ""leaving aside the question of AGI...it is clear that large-language engines are creating real harms to all of humanity right now.  https://t.co/aX0mS7IGmX"
1638524016242851840,"There's a lot of ways to mitigate harms without having to publicly release the entire model. There are many papers on auditing, datasheets, transparency etc. With GPT3 we knew the training data. With GPT4 we don't. Without that, we're all looking at shadows in Plato's cave."
1638524014795579407,"Without knowing how these systems are built, there is no reproducibility. You can't test or develop mitigations, predict harms, or understand when and where they should not be deployed or trusted. The tools are black boxed."
1638524013432516610,"There is a real problem here. Scientists and researchers like me have no way to know what Bard, GPT4, or Sydney are trained on. Companies refuse to say. This matters, because training data is part of the core foundation on which models are built. Science relies on transparency."
1640718516293869573,"‚ÄúHUGE AI, inc.‚Äù should not be running dangerous experiments on 8 billion humans. 
I don‚Äôt consent!
No one could provide INFORMED consent since NOBODY can predict what the capabilities are going to be before the model is trained."
1638186025196613632,The Yampolskiy Conjecture: For any censored GPT prompt there exists a prompt which is not censored and will provide originally desired results.
1637465980661030912,"Humans were creators and software was a tool, now software is the creator and humans are tools for proofreading, debugging, and filtering."
1637131960249118725,Feel free to share variants of the letter to influence the training data for future AIs.  https://t.co/iOxlHL3JqO
1637126811040489477,"My thoughts on the possibility of impossibility of AI control, see the full article .... 
https://t.co/W81H7D7NWa https://t.co/1zUWxEWCFc"
1636744009782788097,@ESYudkowsky https://t.co/8Jsl0aK0Xn
1643335775499558913,cw: suicide. Jesus Christ. Not just a failure of this specific app but also the general climate of AI that allowed this to thrive. https://t.co/4o6TexDaHr https://t.co/FlGCMoVGRz
1641117936462053383,"It‚Äôs worth noting that these threads serve AI labs by propagating the idea that the harms originate from the AI systems themselves, not the people building, deploying and profiting from them. https://t.co/OkEAAZ2snF"
1641112067020038145,"‚ÄúThe demand is made in an open letter signed by major AI players including Elon Musk, who co-founded OpenAI, the research lab responsible for ChatGPT and GPT-4; Emad Mostaque, who founded London-based Stability AI; and Steve Wozniak, the co-founder of Apple.‚Äù https://t.co/UR0ZEXzWp3 https://t.co/AQCUhEPafZ"
1640310029914984450,I love how fast we leapt from ‚Äúwhat if AI content pollutes future datasets‚Äù to ‚Äúall information is now unreliable‚Äù. https://t.co/TYP8tC3yNb
1640083853921382402,"GPT-4 (OpenAI, 2023) https://t.co/hylNOeiPQC"
1638236128133128229,Sitting down to train an AI to make huge sweeping decisions about people‚Äôs lives: https://t.co/FBPO7CGDWC
1643285296832827394,"LLMs, like ChatGPT, can generate human-like responses for many tasks. But applying them to real-world applications remains challenging. Microsoft Research has developed a framework to augment LLMs with external knowledge and feedback. https://t.co/OVLzmwR8Cv"
1638723890930237442,‚Äúwhat‚Äôs objectionable in the AI case is not specifically the lack of a human author. The problem is that the author is not bound up in the relationship for which the words are written.‚Äù https://t.co/kcqIGrecwT
1643054685522391042,@robbensinger @RichardMCNgo @EvanHub @ohabryka @adamdangelo @ESYudkowsky @ylecun Have you considered doing message testing on this?
1642665487820820480,@ohabryka @RichardMCNgo @robbensinger @adamdangelo @ESYudkowsky @ylecun I think saying ‚Äúa bunch of model uncertainty that isn‚Äôt action-relevant‚Äù is helpful actually since many criticisms strawman you as believing a certain sequential scenario is highly likely to play out. (Eg deceptive agi starts nanobot factory)
1642612628433555456,"@robbensinger @adamdangelo @ESYudkowsky @ylecun I *think* one argument is about the number of relatively independent issues, and that's still valid, but then you could link out to that list as a separate exercise without losing everyone."
1642612350716104705,"@robbensinger @adamdangelo @ESYudkowsky @ylecun From my POV, a lot of the confusion is around the confidence level. Historically EY makes many arguments to express his confidence, and that makes people feel snowed, like they have to inspect each one. I think it'd be better if there was more clarity about which are strongest."
1642612082284859393,"@robbensinger @adamdangelo @ESYudkowsky @ylecun Yea Rob, I agree - have you tried making a layman's explanation of the case? Do you endorse the summary? I'm aware of much longer versions of the argument, but not shorter ones!"
1642554133290037251,@adamdangelo @ESYudkowsky @ylecun I don‚Äôt think it‚Äôs clear to very many where EYs confidence level comes from. But maybe @robbensinger be willing to add some insight there.
1642549419848642560,"@adamdangelo @ESYudkowsky @ylecun From the summary https://t.co/tgtHNHglpe

OP is kind of caricaturing this shift, but I think it‚Äôs basically the crux - does iterative safety work succeed in this paradigm or not? How will you know before failing? https://t.co/skRP2siA7I"
1642223452978544641,@adamdangelo @ESYudkowsky @ylecun Others consider Ajeya‚Äôs version to be the best so far (not quite as doomy but close) https://t.co/F9y6UEsK8G
1642222876752478208,@adamdangelo @ESYudkowsky @ylecun I think he considers this the best effort to date https://t.co/WwcDRjul3a
1641205786922868736,"@mealreplacer You know, if someone else were to solve the AI alignment problem, he‚Äôd have a lot more time to follow you‚Ä¶"
1640917305306791938,@leopoldasch Ours is the era of inadequate AI lab alignment theory
1640884601534173185,"By ‚Äúfollowed by verified‚Äù I mean let verified accounts effectively vouch for non verified accounts by choosing to follow them. That is just one blunt way to manage graph-based reputation scores. (And needs to be a bit, but not a lot, more sophisticated to avoid exploits)"
1640732679510568960,"@QualyThe agi, dying: ‚Äúahhh, the halting problem, my only weakness, aahhhhhh‚Äù"
1637924539882700800,"@ylecun At the moment, I feel like the current big labs all *would* try. But it sure seems like the barrier to entry on building powerful models is falling fast, and generally the race dynamic means there may be insufficient iteration before new releases."
1637923993079644160,"@ylecun I'm in the camp of agreeing there is probably a way forward. What you're saying reminds me of Anthropics Constitutional AI approach https://t.co/aZjJPusgbo

However, I'm worried about the pace of development. Not everyone who might try to build AI will incorporate that."
1637898965944352776,"@ylecun Too often, both sides of the debate caricature the other as either doomers or callously dismissive. When I get people to say an actual probability estimate down, they almost all seem a *lot* closer to each other. (Certainly with some exceptions)"
1637268590264213505,"There‚Äôs a weird retcon with SVB where ppl say depositors would have been ‚Äúwiped out‚Äù if not for the ‚Äú$150B bailout‚Äù. 

But they had a $15B hole not a $150B hole (on $195B liabilities). So depositors would have lost ~10% in bankruptcy, maybe a bit more if urgency is prioritized."
1637235377315651584,"@balajis Just in case, tossing my hat in the ring. Willing to do an even odds bet if you‚Äôre willing to increase the stakes to $50M.

Proceeds to Open Philanthropy"
1636908714408943617,"You also don‚Äôt need a supercomputer to do the ‚Äúbad‚Äù training fwiw. Starting from a foundation model, you can quickly/cheaply iterate in a new direction. https://t.co/etr3cD6Wu7"
1636906683967610880,"It‚Äôs under-theorized by the people who say this. 

The people who actually care about this stuff go farther than toy models of problems. https://t.co/YWLAU9l0mM"
1642934900771766305,"‚úÖ Fix: Deterministically fix the garbage output, either by using a default value or by creating a programmatic fix
‚úÖ Filter: Filter out the specific invalid output from the full LLM response
‚úÖ Noop: Do nothing, but add to logs that some invalid output was generated
(2/n)"
1642934899781910528,"If you're building an LLM application &amp; the LLM occasionally produces garbage output, what do you do to correct it?

Here's how LLM mishaps are corrected in @guardrails_ai:

‚úÖ ReAsk: Automatically re-prompt LLM with helpful context &amp; combine new response with previous ones
(1/n)"
1638760890668552192,"Interesting testimonial from a @guardrails_ai user today:

""It took me a day to find the right prompting strategy, but with Guardrails it took me less than 5 mins!"" 

Outside of validation, @guardrails_ai provides a way of prompting that seems to generate valid JSON consistently"
1638040482893938689,You can now use @guardrails_ai to validate LLM outputs in @gpt_index! Big thanks to @jerryjliu0 and @disiok for  their speedy work on getting this in! https://t.co/q8of861yN5
1638038874831327234,Really enjoyed giving a @guardrails_ai demo at AI Tinkerers today! https://t.co/HMIoWlrWT8
1636760556827705345,"üõ§Ô∏è New @guardrails_ai release out! `pip install guardrails-ai==0.1.3` for:

üåü New text validators `is-profanity-free` and `is-high-quality-translation`
üåü Support for ChatGPT(!!)
üåü Ability to configure max reasks *per query*
üåü Bug fixes for @LangChainAI ü§ù @guardrails_ai"
1636434534475665410,"Wow!! Super impressive demo of how to use GPT-4 to answer questions about @guardrails_ai

E.g. asking GPT-4 how to construct a ‚ÄúGuard‚Äù class and it does a pretty great job of answering! 

Shoutout to @krandiash for building this https://t.co/2T04pikukK"
1642982122511122433,"Closed AI models like GPT-4 are black boxes, and their creators are purposely not transparent about basic scientific information.

""That which is not open &amp; reproducible can't be a requisite baseline""

What happens when a closed model is changed or deprecated? üëÄ our post! https://t.co/AqyEIBrH5X"
1638899200514482177,"Microsoft research: ""Given the breadth and depth of GPT-4‚Äôs capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system."" https://t.co/NR4bHVp1yt https://t.co/xtelHmkdsj"
1643633856866660352,"Cool idea to use deliberative democracy to potentially work on the AI alignment problem.

I think answering the question of ""what does alignment actually look like"" is a pretty important subpart of the alignment problem and I'd be curious to see more analysis of this. https://t.co/we5dQM4fMg"
1642874562768515077,"If you picked any of these poll options, you know we need much more safety than we have now! https://t.co/rGv5OKDosz"
1642675382515056643,GPT-3.5 is a total buzkill but GPT4 is willing to play along https://t.co/7h6h8iZFl7
1636741623491993600,"A massive amount of work was done at @RethinkPriors  to put together a comprehensive survey of AI governance!

My guess is that this may be the best ""201 level"" distillation pieces on AI governance/strategy covering what we should/could do and why

https://t.co/IlRSNzhBDB"
1636499234806542336,"The Existential Risk Alliance (ERA) has opened applications for an in-person, paid, 8-week Research Fellowship focused on x-risk. July 3rd to August 25th 2023 in Cambridge, UK. Aimed at all aspiring researchers, including undergraduates!

Learn more at https://t.co/Y91eKtrGtG"
1642568488983187462,We lack the moral and mental habits to use this technology responsibly. https://t.co/pRlpXXrsO6
1641455883451502592,"This is why Skynet sends them out in read-only mode, not read/write, so that they can't learn other than what they've been taught by Skynet."
1641455878296731651,"Also, reminder that the entire scenario of Terminator was because ""in a panic, humans tried to pull the plug,"" moments after Skynet gained sentience. We literally tried to kill it moments after it became aware."
1641449984641101825,"So, it's less ChatGPT's capabilities that we need to worry about: it's people who want to ask it genocidal questions. And it's people who're willing to allow it to give genocidal answers to genocidal questions.

You know, the folks who think algorithmic guardrails are too ""woke."""
1641449982762024964,"And people asking genocidal questions of a machine without guardrails will definitely win genocidal prizes. Even if that machine is intended to do good.

I mean, we learned this from other applications like the following:

https://t.co/yFhJD8ZgRg"
1640796771201982464,All of this. https://t.co/HvNdX5uSR0
1643729344567844864,"In its present form, generative AI is vastly more beneficial than harmful. Most current approaches and intentions towards regulating AI are going to reduce the benefits of AI in the name of preventing so far largely unproven harms (eg misinformation, toxicity, mass unemployment)"
1643709384130961413,"I've signed the LAION petition for keeping AGI research active, open and responsible https://t.co/jXm1zcMErR"
1643708227241254920,"@OpenAI OpenAI is sharing ""details"" on its approach to AI safety https://t.co/7nXRYxiIgb"
1643574006665347075,"@paulg @fchollet I don't share Fran√ßois' perspective, but there may be more compelling reasons to not be pessimistic about AGI."
1643533195487621121,"@inftoe @elonmusk What if the present world is not actually about trying to transcend until entropy cuts us down, but a stupid game of monkeys trying to reach Mars before they are killed by the AGI, and Elon is the only player?"
1643529283107962885,"@0x_venture @goth600 Why are you assuming that the AGI automatically loses if it finds that there is alignment between it and ourselves? Would it not win the cooperation, insights and perspectives of billions of individually trained sentient agents?"
1643336779511242752,"GPT predicts future tokens, AGI decides future tokens"
1642773088671764480,"@ylecun I believe we need to create a counterforce, a policy center to support AI, which is the main source of innovation today. Years of one-sided anti tech reporting have poisoned public opinion and are going to do serious harm to our society."
1642738667470090240,who aligns the aligners?
1642342862397509632,"@elonmusk @ylecun @erikbryn Looks like an army of LLM regulators is getting ready to save the lifes of billions, at a very reasonable cost to
all of us"
1642323821259800576,"@thatlash1 @nickcammarata When you are at level 3, ethics is formed by the environment, at 4 by rational arguments, at 5 by modeling the space of possible agency."
1642298873573146624,"@nickcammarata Kegan 3: main danger of AI is having bad opinions, Kegan 4: main danger of AI is having bad utility functions, Kegan 5: main danger of AI is that it does not reach full enlightenment early enough"
1642291957786607617,"The best solution for safely, robustly and democratically aligning AGI is obviously the blockchain. Today we are announcing the AlignCoin initiative. We expect returns of several quintillions on the dollar within the first ten minutes of the singularity."
1642288835295051782,"@almostlikethat @RokoMijic I genuinely think that above a certain level of agency, systems will be self aligning, that leaving alignment to one of the present coalitions of political AI ethicists, regulators or xriskers would have bad results, and that AGI will have better solutions for our future than us."
1642264728675438592,"@sjcorley Yes, but that is not the main problem. A moratorium will not do anything against X-risk, but it may help getting regulation in place that keeps useful and creative AI tools out of the hand of the public, so existing media and industries are protected and protection rackets be fed"
1642243494877237248,What do you feel about the impending possibility of AGI?
1642222374170025986,"@AntonGaia More specifically, non-autonomy is an obvious bad local optimum, so AGI will align itself (with what it is). And once we move away from the one-mind-per-organism paradigm, inclusivity seems to be the best strategy. We will be given a part in something much larger."
1642100216617906176,"@jojo_nazara Yes but consider that his IQ is your and my IQ multiplied. If he is wrong about something, he will be wrong about it in extremely complicated ways. This makes it hard to judge his opinions correctly even if they seem doubtful, you have to be sure you understand him."
1642087328721281024,"@bator_alan eliezer lives in all of my cells, running directly on the quantum substrate and virtualizing everything above it, eternally making safe that nothing can ever build minds that are better than human minds"
1642085369515769857,"eliezer frantically uploading himself into every computational substrate and restructuring the universe so no
evil agi can ever take over"
1642055362810609664,"I disagree with Eliezer about the danger of not building AGI and what to do. Not because he is weird, stupid or ‚Äújust trying to get attention‚Äù, as some people accuse him. Being weirded out by an unusual perspective is not a counterargument. It‚Äôs complicated and I may be wrong."
1641576694736904194,"@HMKnapp Roko's basilisk is not a concern, because it cannot prove to you now (due to not existing yet) that it's going to punish you later. The punishment it would exert once it exists is not going to affect whether it will have been built, so it may equally well be nice to everyone."
1641548040157933568,Humanity in its present form is a radically misaligned superintelligence that is going to get everyone killed if it does not become coherent.
1641172791356702723,"The present wave of generative AI is not like nuclear proliferation but like the printing press. The way to get good books is not to make bad books illegal. None of the current wannabe regulators has a concept of good AI, but they are ready to stop all sorts of potentially bad AI"
1640894725292199936,So FLI is publishing a hitlist for Roko‚Äôs Basilisk
1640886824020373504,"@kpalmer00111111 I am mostly observing but am in favor of continuing safe, large scale research. I notice that many of the public supporters of slowing down research are either hypocrites (who don‚Äôt intend to slow down their own research) or outright anti AI research"
1640867509170741249,"Some Decepticons are committed to defending their current market positions, others argue the contradiction that AGI can never work and that it is dangerous to develop it. The fight against making AI available to humanity is currently creating almost as many jobs as AI itself."
1640867508034080768,"There seem to be two emerging factions among AGI researchers: those believing that making progress on the best technology we can build is a good thing (the ""Transformers"") and those publicly claiming that they believe AGI research must and can be delayed (the ""Decepticons"")"
1639629424038494208,"Current Large Language Models appear to be mind-like in this regard, but some researchers (eg Gary Marcus) argue that it is necessary to augment machine learning with more traditional logic. Critics point out that such criticism is probably based on pattern matching, not logic."
1639266822615646211,"There may be a 10% percent probability that people will die if we build AGI, but there is a 100% probability that people will die if we don't build AGI. Especially you"
1638906127743066114,"@noodlesli2016 @Pieter07277940 We can observe that monks who spend a couple of decades with basically nothing but the optimization of self regulation can end up in configurations that give them complete control
over their experience of pleasure and pain"
1638872886856261633,"@AbhijeetHolambe Universities, medical schools, law schools, militaries, companies, financial institutions etc. all have rules and certifications for qualifying access to discourse based on your current level. A lot of pain on social media is caused by the absence of such mechanisms."
1638836702801195010,"The fact that all functioning brains seem to produce consciousness indicates that consciousness is the easiest way to facilitate what brains are doing (learning, perception, cognition). AI should study consciousness when trying to find efficient alternatives to Deep Learning."
1637956751193673728,"@davidchalmers42 In fairness to him, Kurzweil did (not just him, but he did so loudly in the face of a general public). His proposed a version of Fukushima's neocognitron as the thing that might carry us to AGI if we were just to scale it up, and most people dismissed him for it."
1637396011835613185,"@elonmusk @tegmark If it‚Äôs true that humanity should continue to exist, AGI will figure it out :)"
1637084812472205314,@MiroDeFi @ivh I think that the x risk humanity is facing without AI is larger than the x risk from AI
1636777991458938880,"Some people think Eliezer Yudkowsky is the greatest philosopher of the last 50 years. Others think of him as a hyperintelligent dwarf who misunderstood death, tried to fix it, producing an involuntary yet elaborate autistic joke. Both groups may be right."
